{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CH05_오차역전파법.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1v6Mtt2XWSXh4Au76KP9VKGhOezBuGp9O","authorship_tag":"ABX9TyPA/ncQiqT/g+zPSCZLHLBm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 5. 오차역전파법\n","신경망이 가중치 매개변수에 대한 손실 함수의 기울기를 수치 미분을 사용해 구했다.   \n","하지만 수치 미분은 단순하지만 계산 시간이 오래 걸린다는 단점이 있다.   \n","가중치 매개변수의 기울기를 효율적으로 계산하는 오차역전파법을 배워보자."],"metadata":{"id":"ouMwKjxbAPWH"}},{"cell_type":"markdown","source":["# 5.1 계산 그래프\n","계산 과정을 그래프로 나타낸것을 __계산 그래프__라고 한다.   \n","그래프 자료구조는 여러 노드와 엣지로 표현된다.   \n","간단한 예시를 통해 차근차근 알아보자."],"metadata":{"id":"ksD9JHkSAV2b"}},{"cell_type":"markdown","source":["### 5.1.1 계산 그래프로 풀다\n","문제 1. 슈퍼에서 1개에 100원인 사과를 2개 샀습니다. 이때 지불 금액을 구하세요. 소비세 10%가 부과됩니다.   \n","![](https://blog.kakaocdn.net/dn/biHgge/btqIyea02FF/u5P7jDQgSCPa9D3RMPGnOK/img.png)"],"metadata":{"id":"ilT8hUWDAYWi"}},{"cell_type":"markdown","source":["문제 2. 슈퍼에서 사과 2개, 귤 3개를 샀습니다. 사과는 1개에 100원, 귤을 1개에 150원 입니다. 소비세가 10%일 때 지불 금액을 구하세요.   \n","![](https://blog.kakaocdn.net/dn/bpjCkU/btqIwZkLAfb/iY8xeXo1deO4RqobY2QKKk/img.png)"],"metadata":{"id":"-lYp5ASxvoDF"}},{"cell_type":"markdown","source":["위의 예제와 같이 계산 그래프를 이용한 문제는 다음 흐름으로 진행된다.   \n","1. 계산 그래프를 구성한다.\n","2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다.\n","\n","여기서 2번에 해당되는 진행 단계를 순전파라고 한다.   \n","반대로 오른쪽에서 왼쪽으로 진행되는 것을 역전파라고 한다."],"metadata":{"id":"Nbuqm7rav5yt"}},{"cell_type":"markdown","source":["### 5.1.2 국소적 계산\n","계산 그래프는 국소적 계산을 전파함으로써 최종 결과를 얻는다.   \n","여기서 국소적이란 자신과 직접 관계된 작은 범위를 말한다.   \n","전체 계산이 아무리 복잡하더라도 각 단계에서 하는 일은 해당 노드의 국소적 계산이다.   \n","국소적인 계산은 단순하지만, 결과를 전달함으로써 결국 복잡한 계산을 해낼 수 있게된다."],"metadata":{"id":"B8TGm8WlAahF"}},{"cell_type":"markdown","source":["### 5.1.3 왜 계산 그래프로 푸는가?\n","계산 그래프는 국소적 계산으로 전체가 아무리 복잡해도 각 노드에서 단순한 계산에 집중하여 문제를 단순화할 수 있다.   \n","그리고 중간 계산 결과를 모두 보관할 수 있다.   \n","가장 큰 이유는 __역전파를 통해 미분을 효율적으로 계산할 수 있다.__   \n","\n","사과 가격이 오르면 최종 금액에 어떤 영향을 끼치는지 알고 싶을때 어떻게 해야할까?   \n","'사과 가격에 대한 지불 금액의 미분'을 구하는 문제에 해당된다.   \n","사과 값을 $x$, 지불 금액을 $L$이라 했을 때 ${{\\alpha L} \\over{\\alpha x}}$를 구하는 것이다.   \n","이는 사과 값이 아주 조금 올랐을 때 지불 금액이 얼마나 증가하는지 표시한 것이다.   "],"metadata":{"id":"HJ8Hf1UjAb3T"}},{"cell_type":"markdown","source":["![](https://mblogthumb-phinf.pstatic.net/MjAxODA2MjNfMTQw/MDAxNTI5NzM2NTEzMzcz.qZSa133HKdSkflstjt9bxI2znQvhRs1c20sxj0RdX7Qg.DI2D8zxzu3fJ5G5dZ_6YVjKn2OaExmjMAg5dQDKlJFsg.PNG.ssdyka/fig_5-5.png?type=w2)\n","\n","역전파 과정을 통해 미분을 한다.   \n","중간까지 구한 결과를 공유할 수 있어서 다수의 미분을 효율적으로 계산할 수 있다.   \n","\n","계산 그래프의 이점을 다시 한번 정리하자면 __순전파와 역전파를 활용해 각 변수의 미분을 효율적으로 구할 수 있다.__"],"metadata":{"id":"yUE383bDyCau"}},{"cell_type":"markdown","source":["# 5.2 연쇄법칙\n","역전파는 국소적인 미분을 오른쪽에서 왼쪽으로 전달한다.   \n","이를 전달하는 원리는 연쇄법칙에 따른 것이다.   \n","연쇄법칙과 이것이 계산 그래프상의 역전파와 같다는 사실을 알아보자."],"metadata":{"id":"wYGK22xoAeuP"}},{"cell_type":"markdown","source":["### 5.2.1 계산 그래프의 역전파\n","$y=f(x)$ 수식의 역전파를 계산 그래프로 확인해보자.   \n","![](https://blog.kakaocdn.net/dn/FggJa/btqIqmVta4z/hDrKdVzzvHX7wAElrQ0Q4K/img.png)\n","\n","신호 $E$의 노드에 국소적 미분 $({\\alpha y \\over{\\alpha x}})$를 곱한 후 다음 노드로 전달하는 것이다.\n","\n","이러한 방식이 연쇄법칙의 원리로 미분 값을 효율적으로 구할 수 있다는 것이 역전파의 핵심이다.   "],"metadata":{"id":"AvAPUn4hAgWz"}},{"cell_type":"markdown","source":["### 5.2.2 연쇄법칙이란?\n","연쇄법칙을 알기 위해 합성 함수부터 알아야 한다.   \n","__합성 함수__는 여러 함수로 구성된 함수이다.   \n","예를 들어 $z = (x+y)^2$는 $z = t^2$와 $t = x+y$로 두 개의 식으로 구성된다.\n","\n","연쇄법칙은 합성 함수의 미분에 대한 성질이고 다음과 같이 나타낼 수 있다.   \n","> 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.\n","\n","위 식은 ${\\alpha z \\over {\\alpha x}}$ (x에 대한 z의 미분)은 ${\\alpha z \\over {\\alpha t}}$ (t에 대한 z의 미분)과 ${\\alpha t \\over {\\alpha x}}$ (x에 대한 t의 미분)의 곱으로 나타낼 수 있다.   \n","\n","${\\alpha z \\over {\\alpha x}} = {\\alpha z \\over {\\alpha t}}{\\alpha t \\over {\\alpha x}}$   \n","여기서 $\\alpha t$를 지울 수 있다.   \n","${\\alpha z \\over {\\alpha x}} = {\\alpha z \\over {\\alpha x}}$   \n","\n","연쇄법칙을 써서 미분 ${\\alpha z \\over {\\alpha x}}$를 구해보자. 먼저 국소적 미분(편미분)을 구한다.   \n","${\\alpha z \\over {\\alpha t}} = 2t$   \n","${\\alpha t \\over {\\alpha x}} = 1$   \n","\n","미분 공식을 통해 해석적으로 구했는데, 최종적으로 구하고 싶은 ${\\alpha z \\over {\\alpha x}}$는 위에서 구한 두 미분을 곱해서 계산하면 된다.   \n","${\\alpha z \\over {\\alpha x}} = {\\alpha z \\over {\\alpha t}} {\\alpha t \\over {\\alpha x}} = 2t * 1 = 2(x+y)$"],"metadata":{"id":"z_24CnBgAijJ"}},{"cell_type":"markdown","source":["### 5.2.3 연쇄법칙과 계산 그래프\n","![](https://blog.kakaocdn.net/dn/9U9J5/btqIvDCbsVv/mS9ZrimZQoD2Ghzu85HIcK/img.png)\n","\n","노드로 들어온 입력 신호에 그 노드의 국소적 미분을 곱한 후 다음 노드로 전달한다.\n","\n","가장 왼쪽의 역전파를 보면 연쇄법칙이 성립되어 $x$에 대한 $z$의 미분이 된다.   \n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FBLeIk%2FbtqInOkpSrO%2FZriGfBidyOqeudtrwgWwnk%2Fimg.png)   \n","계산 그래프의 역전파 결과에 따르면 ${\\alpha z \\over {\\alpha x}}$는 $2(x+y)$임을 확인할 수 있다."],"metadata":{"id":"kX-UYLBJAjxC"}},{"cell_type":"markdown","source":["# 5.3 역전파\n","덧셈과 곱셈 노드에 따른 역전파를 알아보자."],"metadata":{"id":"nX8nR60RAmpX"}},{"cell_type":"markdown","source":["### 5.3.1 덧셈 노드의 역전파\n","$z = x+y$라는 식을 대상으로 역전파를 살펴보자.   \n","위 식의 미분은 다음과 같이 해석적으로 계산할 수 있다.   \n","${\\alpha z \\over {\\alpha x}} = 1$   \n","${\\alpha z \\over {\\alpha y}} = 1$   \n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FweVPG%2FbtqIwZebgfD%2FbM9nRxwgTxym9kabDhoMaK%2Fimg.png)   \n","역전파에서는 상류에서 전해진 미분에 1을 곱하여 하류로 흘린다.   \n","덧셈 노드의 역전파는 1을 곱하기만 하면 되므로 __입력된 값을 그대로 다음 노드로 보낸다.__   \n","\n","미분 값으로 ${\\alpha L \\over {\\alpha z}}$이 됐는데, 이는 최종적으로 $L$이라는 값을 출력하는 큰 계산 그래프를 가정하기 때문이다.   \n","$z=x+y$ 계산은 큰 계산 그래프의 중간 어딘가에 존재하고, 상류로부터 전해져온 값을 의미한다.\n","\n","구체적인 예로 '10+5=15'라는 계산이 있고, 상류에서 1.3이라는 값이 흘러온다. 이는 다음과 같이 표현된다.   \n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwryMw%2FbtqIs83pNrO%2FbQDbak0dicJeaklazsMGgk%2Fimg.png)"],"metadata":{"id":"buim9yaZAoN7"}},{"cell_type":"markdown","source":["### 5.3.2 곱셈 노드의 역전파\n","$z = xy$라는 식을 생각해보자. 이 식의 미분은 다음과 같다.   \n","${\\alpha z \\over {\\alpha x}} = y$   \n","${\\alpha z \\over {\\alpha y}} = x$   \n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcWoZmR%2FbtqIvXU9pEN%2FIiCVTyc4TrfxSJ7pNgjrr0%2Fimg.png)\n","\n","구체적인 예로 '10x5=50'이라는 계산을 역전파로 해보자.   \n","이때 상류에서 1.3 값이 흘러온다.   \n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbhqAsb%2FbtqInOxU0Uh%2FtoTOhOl8x5IXIvMGfrSlkk%2Fimg.png)\n","\n","덧셈의 역전파는 상류의 값을 그대로 흘려보내서 순방향 입력 신호의 값이 필요하지 않았지만, 곱셈의 역전파는 순방향 입력 신호의 값이 필요하다.   \n","그래서 곱셈 노드를 구현할 때는 순전파의 입력 신호를 변수에 저장한다."],"metadata":{"id":"ODZS7AeSAp0C"}},{"cell_type":"markdown","source":["### 5.3.3 사과 쇼핑의 예\n","![](https://mblogthumb-phinf.pstatic.net/MjAxODA5MDlfMjU0/MDAxNTM2NDY5MTYyNzM0.NYHbHx8H7fvCbcRFH3Hd7fkvqiJ6MnrRe8Q_cFoOD90g.Ds-9h4CYQDxU4Y80m1PQDktedGIh6_nq-gWN-uQiZEkg.PNG.ssdyka/fig_5-17.png?type=w2)"],"metadata":{"id":"HHfej0ULA_UY"}},{"cell_type":"markdown","source":["# 5.4 단순한 계층 구현하기\n","![](https://mblogthumb-phinf.pstatic.net/MjAxODA2MjNfMTQw/MDAxNTI5NzM2NTEzMzcz.qZSa133HKdSkflstjt9bxI2znQvhRs1c20sxj0RdX7Qg.DI2D8zxzu3fJ5G5dZ_6YVjKn2OaExmjMAg5dQDKlJFsg.PNG.ssdyka/fig_5-5.png?type=w2)   \n","사과 쇼핑의 예를 파이썬으로 구현해보자.   \n","계산 그래프의 곱셈 노드를 MulLayer, 덧셈 노드를 AddLayer라는 이름으로 구현해보자."],"metadata":{"id":"xnlcbf_eBBRt"}},{"cell_type":"markdown","source":["### 5.4.1 곱셈 계층\n","곱셈 계층을 구현해보자."],"metadata":{"id":"rK2uS9VnBCSc"}},{"cell_type":"code","source":["class MulLayer:\n","    def __init__(self):\n","        self.x = None\n","        self.y = None\n","    \n","    def forward(self, x, y):\n","        self.x = x\n","        self.y = y\n","        out = x * y\n","        return out\n","    \n","    def backward(self, dout):\n","        dx = dout * self.y\n","        dy = dout * self.x\n","        return dx, dy"],"metadata":{"id":"PbAS8aseK4jU","executionInfo":{"status":"ok","timestamp":1658136488370,"user_tz":-540,"elapsed":8,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["apple = 100\n","apple_num = 2\n","tax = 1.1\n","\n","# layer\n","mul_apple_layer = MulLayer()\n","mul_tax_layer = MulLayer()\n","\n","# 순전파\n","apple_price = mul_apple_layer.forward(apple, apple_num)\n","price = mul_tax_layer.forward(apple_price, tax)\n","\n","print(price) # 220"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"92iSUXgoLS56","executionInfo":{"status":"ok","timestamp":1658136488370,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"53d6b86f-a01f-4481-c4df-9ccd4413abdc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["220.00000000000003\n"]}]},{"cell_type":"code","source":["# 역전파\n","dprice = 1\n","dapple_price, dtax = mul_tax_layer.backward(dprice)\n","dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n","\n","print(dapple, dapple_num, dtax)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B6lDOnfuP1B9","executionInfo":{"status":"ok","timestamp":1658136488370,"user_tz":-540,"elapsed":5,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"1b2eef25-597e-45cb-cd84-e22c09901139"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["2.2 110.00000000000001 200\n"]}]},{"cell_type":"markdown","source":["backward()가 받는 인수는 순전파의 출력에 대한 미분임을 주의하자."],"metadata":{"id":"Ns-lX8O_QO_V"}},{"cell_type":"markdown","source":["### 5.4.2 덧셈 계층\n","덧셈 계층을 구현해보자."],"metadata":{"id":"Eoy1uJTtBD5T"}},{"cell_type":"code","source":["class AddLayer:\n","    def __init__(self):\n","        pass\n","    \n","    def forward(self, x, y):\n","        out = x + y\n","        return out\n","    \n","    def backward(self, dout):\n","        dx = dout * 1\n","        dy = dout * 1\n","        return dx, dy"],"metadata":{"id":"n03aaNd0URAu","executionInfo":{"status":"ok","timestamp":1658136488370,"user_tz":-540,"elapsed":3,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["![](https://mblogthumb-phinf.pstatic.net/MjAxODA5MDlfMjU0/MDAxNTM2NDY5MTYyNzM0.NYHbHx8H7fvCbcRFH3Hd7fkvqiJ6MnrRe8Q_cFoOD90g.Ds-9h4CYQDxU4Y80m1PQDktedGIh6_nq-gWN-uQiZEkg.PNG.ssdyka/fig_5-17.png?type=w2)\n","\n","위 이미지 상황을 구현해보자."],"metadata":{"id":"WZkRqMpvUh22"}},{"cell_type":"code","source":["apple = 100\n","apple_num = 2\n","orange = 150\n","orange_num = 3\n","tax = 1.1\n","\n","# layer\n","mul_apple_layer = MulLayer()\n","mul_orange_layer = MulLayer()\n","add_apple_orange_layer = AddLayer()\n","mul_tax_layer = MulLayer()\n","\n","# forward\n","apple_price = mul_apple_layer.forward(apple, apple_num)\n","orange_price = mul_orange_layer.forward(orange, orange_num)\n","all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n","price = mul_tax_layer.forward(all_price, tax)\n","\n","# backward\n","dprice = 1\n","dall_price, dtax = mul_tax_layer.backward(dprice)\n","dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n","dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n","dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n","\n","print(price)\n","print(dapple_num, dapple, dorange, dorange_num, dtax)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUakRRnmUjiW","executionInfo":{"status":"ok","timestamp":1658136488943,"user_tz":-540,"elapsed":576,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"e2d55381-fe40-409e-8b2b-518e98076dcc"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["715.0000000000001\n","110.00000000000001 2.2 3.3000000000000003 165.0 650\n"]}]},{"cell_type":"markdown","source":["# 5.5 활성화 함수 계층 구현하기\n","계산 그래프를 신경망에 적용해보자.   \n","신경망을 구성하는 layer 각각을 클래스 하나로 구현하자.   \n","먼저 활성화 함수인 ReLU와 Sigmoid를 구현해보자."],"metadata":{"id":"-2ONX-fVBF0J"}},{"cell_type":"markdown","source":["### 5.5.1 ReLU 계층\n","![](https://blog.kakaocdn.net/dn/vgJna/btqQzRGmwcO/TK3KTMlz4CYag8rBTKfYkK/img.png)\n","\n","순전파에서 $x$가 0보다 크면 역전파는 상류의 값을 그대로 하류로 흘린다.   \n","$x$가 0보다 작으면 하류로 0을 보낸다."],"metadata":{"id":"jje2ul9pBHNL"}},{"cell_type":"code","source":["class ReLU:\n","    def __init__(self):\n","        self.mask = None\n","    \n","    def forward(self, x):\n","        self.mask = (x <= 0)\n","        out = x.copy()\n","        out[self.mask] = 0\n","        return out\n","    \n","    def backward(self, dout):\n","        dout[self.mask] = 0\n","        dx = dout\n","        return dx"],"metadata":{"id":"xfMwc3DDWsJ4","executionInfo":{"status":"ok","timestamp":1658136488943,"user_tz":-540,"elapsed":8,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["mask라는 인스턴스 변수를 가진다.   \n","이는 True, False로 구성된 넘파이 배열로 순전파 입력인 x의 원소 값이 0 이하인 인덱스는 True, 그외는 False로 유지한다.\n","\n","역전파는 순전판때 만들어둔 mask를 써서, mask의 원소가 True일 경우에 상류에서 전파된 dout을 0으로 설정한다."],"metadata":{"id":"F5P41T3aW8ST"}},{"cell_type":"code","source":["import numpy as np\n","x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n","print(x)\n","mask = (x <= 0)\n","print(mask)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"byiE0cXyW7fr","executionInfo":{"status":"ok","timestamp":1658136488944,"user_tz":-540,"elapsed":8,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"c7193091-381d-4ffe-e420-a1bfa64ba9d1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 1.  -0.5]\n"," [-2.   3. ]]\n","[[False  True]\n"," [ True False]]\n"]}]},{"cell_type":"markdown","source":["### 5.5.2 Sigmoid 계층\n","![](https://t1.daumcdn.net/cfile/tistory/275BAD4F577B669920)\n","\n"],"metadata":{"id":"1soJjurUBJSo"}},{"cell_type":"markdown","source":["시그모이드 함수는 다음 식을 의미한다.   \n","$y = {1\\over{1+exp(-x)}}$\n","\n","시그모이드 계층의 계산 그래프는 '*', '+', 'exp', '/' 노드가 등장한다.   \n","'exp' 노드는 $y = exp(x)$ 계산을 수행하고 '/' 노드는 $y = {1\\over{x}}$ 계산을 수행한다.\n","\n","![](https://velog.velcdn.com/images%2Flilpark%2Fpost%2Fe4fc1d53-2443-477e-a9b0-3e5a9fae8eb3%2Fimage.png)\n","\n","이를 계산 그래프에서 간소화하면 다음과 같이 나온다.   \n","![](https://velog.velcdn.com/images%2Flilpark%2Fpost%2F2b877b61-b267-4852-9022-a9da55c1db00%2Fimage.png)"],"metadata":{"id":"ZrRqoopJ92SM"}},{"cell_type":"markdown","source":["${\\alpha L \\over {\\alpha y}}y^2 exp(-x)$   \n","$= {\\alpha L \\over {\\alpha y}}{1 \\over {(1+exp(-x))^2}}exp(-x)$   \n","$= {\\alpha L \\over {\\alpha y}}{1 \\over {1 + exp(-x)}} {exp(-x) \\over 1+exp(-x)}$   \n","$= {\\alpha L \\over {\\alpha y}}y(1-y)$\n","\n","위 계산식을 보면 sigmoid 계층의 역전파는 순전파의 출력 $y$의 값만으로 계산을 진행할 수 있게 된다. 그래서 다음과 같이 표현할 수 있게 된다.   \n","\n","![](https://blog.kakaocdn.net/dn/bR5ka7/btrbN5cJFxP/u0hZweWlmhIHq6oGovvK51/img.png)"],"metadata":{"id":"BgLLtiRZ_B52"}},{"cell_type":"code","source":["class Sigmoid:\n","    def __init__(self):\n","        self.out = None\n","    \n","    def forward(self, x):\n","        out = 1 / (1 + np.exp(-x))\n","        self.out = out\n","\n","        return out\n","    \n","    def backward(self, dout):\n","        dx = dout * (1.0 - self.out) * self.out\n","\n","        return dx"],"metadata":{"id":"xwiCoWqlAvjd","executionInfo":{"status":"ok","timestamp":1658136488944,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["위 구현에서는 순전파의 출력을 out 변수에 보관했다가, 역전파 계산할 때 그 값을 사용한다."],"metadata":{"id":"pNSTcratA-jw"}},{"cell_type":"markdown","source":["# 5.6 Affine/Softmax 계층 구현하기"],"metadata":{"id":"I_YnWXKdBM3f"}},{"cell_type":"markdown","source":["### 5.6.1 Affine 계층\n","신경망의 순전파에서 가중치 신호의 총합을 계산하기 위해 행렬의 곱(np.dot())을 사용했다.   \n","예시를 확인해보자."],"metadata":{"id":"-A5-BWRVBWTu"}},{"cell_type":"code","source":["X = np.random.rand(2) # 입력\n","W = np.random.rand(2,3) # weight\n","B = np.random.rand(3) # bias\n","\n","print(X.shape)\n","print(W.shape)\n","print(B.shape)\n","\n","Y = np.dot(X, W) + B\n","print(Y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n81i5SPnB59E","executionInfo":{"status":"ok","timestamp":1658136488944,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"f4583ac4-742b-45cf-b111-baba9d9232f3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["(2,)\n","(2, 3)\n","(3,)\n","(3,)\n"]}]},{"cell_type":"markdown","source":["행렬곱의 핵심은 대응하는 차원의 원소 수를 일치시키는 게 핵심이다.   \n","\n","> 신경망의 순전파 때 수행하는 행렬의 곱을 기하학에서 __어파인 변환(affine transformation)__ 이라고 한다.   \n","그리고 이것을 Affine 계층이라는 이름으로 구현해보자.\n","\n","지금까지 계산 그래프의 노드에 스칼라값이 있었는데 이번엔 행렬이 흐르고 있다.   \n","여기에 역전파에 대해 생각해보자.   \n","행렬을 사용한 역전파도 행렬의 원소마다 전개해보면 스칼라값을 사용한 지금까지의 계산 그래프와 같은 순서로 생각할 수 있다.   \n","\n","${\\alpha L \\over {\\alpha X}} = {\\alpha L \\over {\\alpha Y}} W^T$   \n","${\\alpha L \\over {\\alpha L}} = X^t {\\alpha L \\over {\\alpha Y}}$   \n","\n","여기서 $T$는 전치행렬을 뜻한다.   \n","이는 $W$의 $(i, j)$ 위치의 원소를 $(j, i)$ 위치로 바꾼 것을 말한다.   \n","\n","![](https://blog.kakaocdn.net/dn/bKEIN4/btqIs83DdjS/XITy2ucBrqdnn8prfmiNZK/img.png)   \n","\n","스칼라값이 아닌 행렬 곱을 진행하기 위해 행렬의 형상에 주의해야한다.   "],"metadata":{"id":"bj68xRe1Cd-S"}},{"cell_type":"markdown","source":["### 5.6.2 배치용 Affine 계층\n","데이터 N개를 묶어서 순전파하는 경우인 배치용 Affine 계층을 생각해보자.   \n","\n","![](https://blog.kakaocdn.net/dn/cTcDvH/btqIxY66Rej/LGESmiYoG0tY32Ls2KyNvK/img.png)\n","\n","입력인 $X$의 형상이 (N,2)가 됐다.   \n","\n","편향을 더할 때 주의해야한다.   \n","순전파의 편향 덧셈은 $XW$에 대한 평향이 각 데이터에 더해진다."],"metadata":{"id":"rj59qEUIBZIq"}},{"cell_type":"code","source":["X_dot_W = np.array([[0,0,0], [10,10,10]])\n","B = np.array([1,2,3])\n","\n","print(X_dot_W)\n","\n","print(X_dot_W + B)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jpOm-XekJpH5","executionInfo":{"status":"ok","timestamp":1658136488944,"user_tz":-540,"elapsed":5,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"e555acd3-b6d2-4210-e001-7cebfe38befc"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0  0  0]\n"," [10 10 10]]\n","[[ 1  2  3]\n"," [11 12 13]]\n"]}]},{"cell_type":"markdown","source":["순전파에서 편향 덧셈은 각각의 데이터에 더해진다.   \n","그래서 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 한다."],"metadata":{"id":"W3TQWoHTJzr2"}},{"cell_type":"code","source":["dY = np.array([[1,2,3], [4,5,6]])\n","print(dY)\n","\n","dB = np.sum(dY, axis=0)\n","print(dB)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxNtw1uOJzIp","executionInfo":{"status":"ok","timestamp":1658136488944,"user_tz":-540,"elapsed":4,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"f3f77421-e914-4ce3-fc32-21bd50d5314a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1 2 3]\n"," [4 5 6]]\n","[5 7 9]\n"]}]},{"cell_type":"markdown","source":["이 예에서 데이터가 2개라고 가정했다.   \n","편향의 역전파는 두 데이터에 대한 미분을 데이터마다 더해서 구한다.   \n","그래서 np.sum()에서 0번째축에 대해 총합을 구하는 것이다.   \n","\n","Affine 구현을 확인해보자."],"metadata":{"id":"w4B3OB8TKEcF"}},{"cell_type":"code","source":["class Affine:\n","    def __init__(self, W, b):\n","        self.W = W\n","        self.b = b\n","        self.x = None\n","        self.dW = None\n","        self.db = None\n","\n","    def forward(self, x):\n","        self.x = x\n","        out = np.dot(x, self.W) + self.b\n","\n","        return out\n","    \n","    def backward(self, dout):\n","        dx = np.dot(dout, self.W.T)\n","        self.dW = np.dot(self.x.T, dout)\n","        self.db = np.sum(dout, axis=0)\n","\n","        return dx"],"metadata":{"id":"aMOUK3dD1pDc","executionInfo":{"status":"ok","timestamp":1658136488944,"user_tz":-540,"elapsed":4,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### 5.6.3 Softmax-with-Loss 계층\n","출력층에서 사용하는 소프트맥스 함수에 대해 알아보자.   \n","소프트맥스 함수는 입력 값을 정규화하여 출력한다.   \n","\n","손글씨 숫자 인식에서 다음과 같은 순서로 진행한다.   \n","> 입력 이미지가 Affine 계층과 ReLU 계층을 통과하며 변환하고, 마지막 Softmax 계층에서 10개(숫자 0~9)의 입력이 정규화된다.   \n","최종적으로 Softmax 결과의 값이 확률이 된다.\n","\n","> 신경망은 학습과 추론 작업으로 나뉜다.   \n","추론은 일반적으로 Softmax를 사용하지 않는다.   \n","추론할 때는 마지막 Affine 계층의 출력을 인식 결과로 이용한다.   \n","신경망에서 정규화하지 않는 출력 결과를 __점수(score)__라고 한다.   \n","신경망 추론에서 답을 하나만 내는 경우에는 가장 높은 저무만 알면 되니 Softmax는 필요 없어진다.\n","\n","소프트맥스 레이어를 구현하기 위해 손실 함수의 교차 엔트로피 오차도 포함하여 구현해보자.   \n","먼저 softmax-with-loss 계층의 계산 그래프를 살펴보자.   \n","![](https://i.imgur.com/UPTYfXt.png)"],"metadata":{"id":"M8SXZX-WBbLW"}},{"cell_type":"markdown","source":["그리고 아래와 같이 간소화 할 수 있다.   \n","![](https://velog.velcdn.com/images/bbirong/post/018ee92a-af64-4239-b277-7ddf3add9e16/image.png)"],"metadata":{"id":"Koan1ijXX5MY"}},{"cell_type":"markdown","source":["3개의 클래스 분류를 가정하고 이전 계층에서 3개의 입력(점수)를 받는다.   \n","Softmax 계층은 입력 $(a_1, a_2, a_3)$을 정규화하여 $(y_1, y_2, y_3)$을 출력한다.   \n","Cross Entropy Error 계층은 Softmax의 출력과 정답 테이블 $(t_1, t_2, t_3)$을 받고, 이 데이터들로부터 손실 $L$을 출력한다.   \n","\n","역전파의 결과를 보면 Softmax 계층은 $(y_1-t_1, y_2-t_2, y_3-t_3)$이라는 말끔한 결과를 얻는다.   \n","이는 Softmax 계층의 출력과 정답 레이블의 차분이다.   \n","신경망의 역전파에선 오차가 앞 계층에 전해진다.   \n","__이는 신경망 학습의 중요한 성질이다.__\n","\n","신경망 학습의 목적은 신경망의 출력이 정답 레이블과 가까워지도록 가중치 매개변수의 값을 조정하는 것이다.   \n","그래서 신경망의 출력과 정답 레이블의 오차를 효율적으로 앞 계층에 전달해야한다.   \n","그래서 역전파를 통해 얻는 결과는 Softmax 계층의 출력과 정답 레이블의 차이로, 신경망의 현재 출력과 정답 레이블의 오차를 있는 그대로 드러내는 것이다.   \n","\n","> Softmax 함수의 손실 함수로 Cross Entropy Error를 사용하니 역전파가 말끔히 떨어진다.   \n","이는 우연이 아닌 그렇게 설계되었기 때문이다.   \n","회귀의 출력층에서 사용하는 항등 함수의 손실 함수로 오차제곱합을 이용하는 이유도 이와 동일하다.\n","\n","구체적인 예를 들어보자.   \n","정답 레이블이 (0, 1, 0)이고 Softmax가 (0.3, 0.2, 0.5)를 출력했다.   \n","정답 인덱스에 맞는 확률이 20%로 나와 신경망은 제대로 인식하지 못함을 말한다.   \n","이 경우 Softmax의 역전파는 (0.3, -0.8, 0.5)라는 커다란 오차를 전파한다.   \n","그래서 Softmax의 앞 계층들은 큰 오차로부터 큰 깨달음을 얻게 된다.   \n","\n","정답 레이블이 (0, 1, 0), Softmax가 (0.01, 0.99, 0)일 때를 가정해보자.   \n","이 경우 Softmax 계층의 역전파가 보내는 오차는 비교적 작은 (0.01, -0.01, 0)이 된다.   \n","앞 계층으로 전달된 오차가 작으므로 학습하는 정도도 작아진다.\n","\n","Softmax-with_Loss 계층을 구현한 코드를 확인해보자."],"metadata":{"id":"-weR0xU6YKUb"}},{"cell_type":"code","source":["def softmax(x):\n","    x = x - np.max(x, axis=-1, keepdims=True)\n","    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n","\n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","        \n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","             \n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"],"metadata":{"id":"cuzwFXZXbx_h","executionInfo":{"status":"ok","timestamp":1658136488944,"user_tz":-540,"elapsed":4,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class SoftmaxWithLoss:\n","    def __init__(self):\n","        self.loss = None # 손실\n","        self.y = None # Softmax 출력\n","        self.t = None # 정답 레이블\n","\n","    def forward(self, x, t):\n","        self.t = t\n","        self.y = softmax(x)\n","        self.loss = cross_entropy_error(self.y, self.t)\n","        return self.loss\n","\n","    def backward(self, dout=1):\n","        batch_size = self.t.shape[0]\n","        dx = (self.y - self.t) / batch_size\n","        return dx"],"metadata":{"id":"PvWqOIFebANs","executionInfo":{"status":"ok","timestamp":1658136488944,"user_tz":-540,"elapsed":3,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["역전파 때는 전파하는 값을 배치의 수로 나눠서 데이터 1개당 오차를 앞 계층으로 전파하는 점을 주의하자."],"metadata":{"id":"iLrA_-wbbhI2"}},{"cell_type":"markdown","source":["# 5.7 오차역전파법 구현하기\n","지금까지 구현한 계층들을 조합해서 신경망을 구축해보자."],"metadata":{"id":"siyO-osRBe0i"}},{"cell_type":"markdown","source":["### 5.7.1 신경망 학습의 전체 그림\n","신경망 학습의 순서를 복습해보자.   \n","\n","__전제__   \n","신경망에는 적응이 가능한 가중치와 편향이 있고, 이들을 훈련 데이터에적응하도록 조정하는 과정을 학습이라고 한다. 신경망 학습은 4단계로 수행한다.   \n","__1단계 - 미니배치__   \n","훈련 데이터중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표이다.   \n","__2단계 - 기울기 산출__   \n","미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.   \n","__3단계 - 매개변수 갱신__   \n","가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.   \n","__4단계 - 반복__   \n","1~3단계를 반복한다.   \n","\n","오차역전파법은 두 번째 단계에 해당한다.   \n","앞 장에서 기울기를 구하기 위해 수치 미분을 사용했다.   \n","구현이 쉽지만 계산이 오래 걸린다는 단점이 있다.   \n","그래서 오차역전파법을 사용하여 기울기를 효율적이고 빠르게 구할 수 있게 됐다.\n"],"metadata":{"id":"SJaaMwjKBglB"}},{"cell_type":"markdown","source":["### 5.7.2 오차역전파법을 적용한 신경망 구현하기\n","2층 신경망을 TwoLayerNet 클래스로 구현해보자."],"metadata":{"id":"OJyjQicqBiAM"}},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/Othercomputers/MacBook_Air/Deep_Learning_from_Scratch/V_01/common')"],"metadata":{"id":"vqYRjHMs1dAM","executionInfo":{"status":"ok","timestamp":1658136488945,"user_tz":-540,"elapsed":4,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# import sys, os\n","# sys.path.append(os.pardir)\n","# from common.layers import *\n","# from common.gradient import numerical_gradient\n","\n","import numpy as np\n","import layers\n","from layers import *\n","import gradient\n","from gradient import *\n","\n","from collections import OrderedDict\n","\n","class TwoLayerNet:\n","    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n","        # 가중치 초기화\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","        self.params['b2'] = np.zeros(output_size)\n","\n","        # 계층 생성\n","        self.layers = OrderedDict()\n","        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n","        self.layers['Relu1'] = Relu()\n","        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n","\n","        self.lastLayer = SoftmaxWithLoss()\n","    \n","    def predict(self, x):\n","        for layer in self.layers.values():\n","            x = layer.forward(x)\n","        return x\n","    \n","    # x: 입력 데이터, t: 정답 레이블\n","    def loss(self, x , t):\n","        y = self.predict(x)\n","        return self.lastLayer.forward(y, t)\n","    \n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        if t.ndim != 1 : t = np.argmax(t, axis=1)\n","\n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","    \n","    def numerical_gradient(self, x, t):\n","        loss_W = lambda W: self.loss(x, t)\n","\n","        grads = {}\n","        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","        return grads\n","    \n","    def gradient(self, x, t):\n","        # 순전파\n","        self.loss(x, t)\n","\n","        # 역전파\n","        dout = 1\n","        dout = self.lastLayer.backward(dout)\n","\n","        layers = list(self.layers.values())\n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","        \n","        # 결과 저장\n","        grads = {}\n","        grads['W1'] = self.layers['Affine1'].dW\n","        grads['b1'] = self.layers['Affine1'].db\n","        grads['W2'] = self.layers['Affine2'].dW\n","        grads['b2'] = self.layers['Affine2'].db\n","\n","        return grads"],"metadata":{"id":"HDPcJq5IkOua","executionInfo":{"status":"ok","timestamp":1658137509751,"user_tz":-540,"elapsed":327,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["`__init__`의 계층 생성, `predict`, `gradient`의 순전파와 역전파 부분을 집중해서 살펴보자.   \n","특히 신경망의 계층을 OrderedDict에 보관하는 점이 중요하다.   \n","이는 순서가 있는 딕셔너리이다.   \n","순서가 있다는 뜻은 추가한 순서를 기억한다는 것이다.   \n","그래서 순전파 때는 추가한 순서대로 각 계층의 forward() 메서드를 호출하기만 하면 처리가 완료된다.   \n","역전파 때는 계층을 반대 순서로 호출하기만 하면 된다.   \n","Affine 계층과 ReLU 계층이 각자의 내부에서 순전파와 역전파를 제대로 처리하니, 여기서는 계층을 올바른 순서대로 연결한 다음 순서대로(역전파는 역순으로) 호출해주면 끝이다.   \n","\n","이렇게 신경망의 구성 요소를 계층으로 구현한 덕분에 신경망을 쉽게 구축할 수 있게 됐다.   \n","계층으로 모듈화해서 구현한 효과는 5, 10, 15층과 같이 깊은 신경망을 만들고 싶을 때 필요한 만큼 계층을 추가하면 된다.   \n","\n","이제 각 계층 내부에 구현된 순전파와 역전파를 활용해 인식 처리와 학습에 필요한 기울기를 구해보자."],"metadata":{"id":"MH5SJTQrtgX-"}},{"cell_type":"markdown","source":["### 5.7.3 오차역전파법으로 구한 기울기 검증하기\n","지금까지 기울기 구하는 방법을 두 가지 설명했다.   \n","하나는 수치 미분을 써서 구하는 방법과, 하나는 해석적으로 수식을 풀어 구하는 방법이다.   \n","해석적 방법은 오차역전파법을 이용하여 매개변수가 많아도 효율적으로 계산할 수 있었다.   \n","그래서 이제 수치 미분 대신 오차역전파법을 사용하자.\n","\n","수치 미분은 느리다. 하지만 오차역전파법을 정확히 구현했는지 확인하기 위해 필요하다.   \n","수치 미분의 이점인 구현하기 쉽다는 점을 이용하면 된다.   \n","수치 미분의 구현에는 버그가 숨어 있기 어려운 반면, 오차역전파법은 구현하기 복잡해서 종종 실수를 하곤 한다.   \n","그래서 둘의 결과를 비교하여 오차역전파법을 제대로 구현했는지 검증하곤 한다.   \n","이처럼 두 방식으로 구한 기울기가 일치함을 확인하는 작업을 __기울기 확인(Gradient check)__이라고 한다.   \n","기울기 확인은 다음과 같이 구현한다."],"metadata":{"id":"f6bAQEMABk9q"}},{"cell_type":"markdown","source":["### MNIST dataset 불러오기"],"metadata":{"id":"HxIba_mufjJp"}},{"cell_type":"code","source":["# coding: utf-8\n","try:\n","    import urllib.request\n","except ImportError:\n","    raise ImportError('You should use Python 3.x')\n","import os.path\n","import gzip\n","import pickle\n","import os\n","import numpy as np\n","\n","\n","url_base = 'http://yann.lecun.com/exdb/mnist/'\n","key_file = {\n","    'train_img':'train-images-idx3-ubyte.gz',\n","    'train_label':'train-labels-idx1-ubyte.gz',\n","    'test_img':'t10k-images-idx3-ubyte.gz',\n","    'test_label':'t10k-labels-idx1-ubyte.gz'\n","}\n","\n","dataset_dir = os.path.dirname('/content/drive/Othercomputers/MacBook_Air/Deep_Learning_from_Scratch/V_01/dataset')\n","save_file = dataset_dir + \"/mnist.pkl\"\n","\n","train_num = 60000\n","test_num = 10000\n","img_dim = (1, 28, 28)\n","img_size = 784\n","\n","\n","def _download(file_name):\n","    file_path = dataset_dir + \"/\" + file_name\n","\n","    if os.path.exists(file_path):\n","        return\n","\n","    print(\"Downloading \" + file_name + \" ... \")\n","    headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\"}\n","    request = urllib.request.Request(url_base+file_name, headers=headers)\n","    response = urllib.request.urlopen(request).read()\n","    with open(file_path, mode='wb') as f:\n","        f.write(response)\n","    print(\"Done\")\n","\n","def download_mnist():\n","    for v in key_file.values():\n","       _download(v)\n","\n","def _load_label(file_name):\n","    file_path = dataset_dir + \"/\" + file_name\n","\n","    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n","    with gzip.open(file_path, 'rb') as f:\n","            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n","    print(\"Done\")\n","\n","    return labels\n","\n","def _load_img(file_name):\n","    file_path = dataset_dir + \"/\" + file_name\n","\n","    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n","    with gzip.open(file_path, 'rb') as f:\n","            data = np.frombuffer(f.read(), np.uint8, offset=16)\n","    data = data.reshape(-1, img_size)\n","    print(\"Done\")\n","\n","    return data\n","\n","def _convert_numpy():\n","    dataset = {}\n","    dataset['train_img'] =  _load_img(key_file['train_img'])\n","    dataset['train_label'] = _load_label(key_file['train_label'])\n","    dataset['test_img'] = _load_img(key_file['test_img'])\n","    dataset['test_label'] = _load_label(key_file['test_label'])\n","\n","    return dataset\n","\n","def init_mnist():\n","    download_mnist()\n","    dataset = _convert_numpy()\n","    print(\"Creating pickle file ...\")\n","    with open(save_file, 'wb') as f:\n","        pickle.dump(dataset, f, -1)\n","    print(\"Done!\")\n","\n","def _change_one_hot_label(X):\n","    T = np.zeros((X.size, 10))\n","    for idx, row in enumerate(T):\n","        row[X[idx]] = 1\n","\n","    return T\n","\n","\n","def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n","    if not os.path.exists(save_file):\n","        init_mnist()\n","\n","    with open(save_file, 'rb') as f:\n","        dataset = pickle.load(f)\n","\n","    if normalize:\n","        for key in ('train_img', 'test_img'):\n","            dataset[key] = dataset[key].astype(np.float32)\n","            dataset[key] /= 255.0\n","\n","    if one_hot_label:\n","        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n","        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n","\n","    if not flatten:\n","         for key in ('train_img', 'test_img'):\n","            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n","\n","    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label'])\n","\n","\n","if __name__ == '__main__':\n","    init_mnist()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pk6tRnNVflSe","executionInfo":{"status":"ok","timestamp":1658137511968,"user_tz":-540,"elapsed":574,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"5941cbd0-43d7-4898-8d18-fe1fd4c48f36"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Converting train-images-idx3-ubyte.gz to NumPy Array ...\n","Done\n","Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n","Done\n","Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n","Done\n","Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n","Done\n","Creating pickle file ...\n","Done!\n"]}]},{"cell_type":"code","source":["import numpy as np\n","# from dataset.mnist import load_mnist\n","# import mnist\n","# from mnist import load_mnist\n","# mnist는 colab 환경에서 불러오기 위해 바로 위 코드 블럭 참고\n","\n","# from two_layer_net import TwoLayerNet\n","# TwoLayerNet은 위에 구현된 class 참고\n","\n","# 데이터 읽기\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","x_batch = x_train[:3]\n","t_batch = t_train[:3]\n","\n","grad_numerical = network.numerical_gradient(x_batch, t_batch)\n","grad_backprop = network.gradient(x_batch, t_batch)\n","\n","for key in grad_numerical.keys():\n","    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n","    print(key + \":\" + str(diff))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDbLAj5O5R07","executionInfo":{"status":"ok","timestamp":1658137675684,"user_tz":-540,"elapsed":9391,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"413538e0-630b-4dc3-8e98-30f259df51d3"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["W1:3.876901473863863e-10\n","b1:2.2227473868517007e-09\n","W2:5.7030621614241706e-09\n","b2:1.3987164533824093e-07\n"]}]},{"cell_type":"markdown","source":["MNIST 데이터셋을 읽은 다음 일부 훈련 데이터를 수치 미분으로 구한 기울기와 오차역전파법으로 구한 기울기의 오차를 확인한다.   \n","여기에 각 가중치 매개변수 차이의 절댓값을 구하고, 이를 평균한 값이 오차가 된다.   \n","\n","코드의 실행 결과를 보면 수치 미분과 오차역전파법으로 구한 기울기의 차이가 매우 작다고 말해준다.   \n","> 수치 미분과 오차역전파법의 결과 오차가 0이 되는 일은 드물다.   \n","이는 컴퓨터가 계산할 수 있는 정밀도가 유한하기 때문이다.   \n","정밀도의 한계 때문에 오차는 대부분 0이 되지 않지만, 올바르게 구현했다면 0에 아주 가까운 작은 값이 된다.   \n","이 값이 크면 오차역전파법을 잘못 구현했다고 의심해봐야 한다."],"metadata":{"id":"9SdubFyktSuB"}},{"cell_type":"markdown","source":["### 5.7.4 오차역전바법을 사용한 학습 구현하기"],"metadata":{"id":"igrQBbjTBnB-"}},{"cell_type":"code","source":["# import sys, os\n","# sys.path.append(os.pardir)\n","# import numpy as np\n","# from dataset.mnist import load_mnist\n","# from two_layer_net import TwoLayerNet\n","\n","# 데이터 읽기\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","iters_num = 10000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate = 0.1\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","\n","    # 오차역전파법으로 기울기 구하기\n","    grad = network.gradient(x_batch, t_batch)\n","\n","    # 갱신\n","    for key in ('W1', 'b1', 'W2', 'b2'):\n","        network.params[key] -= learning_rate * grad[key]\n","    \n","    loss = network.loss(x_batch, t_batch)\n","    train_loss_list.append(loss)\n","\n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(x_train, t_train)\n","        test_acc = network.accuracy(x_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","        print(train_acc, test_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gq9n3CZ9uMtK","executionInfo":{"status":"ok","timestamp":1658138584997,"user_tz":-540,"elapsed":39984,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"c3ee45e4-aa7d-46c0-f18d-af36f7819a66"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["0.11915 0.1225\n","0.9008166666666667 0.9029\n","0.9258 0.9253\n","0.9366166666666667 0.9364\n","0.9455333333333333 0.9452\n","0.9494833333333333 0.9475\n","0.9551166666666666 0.9522\n","0.9585666666666667 0.9575\n","0.9621 0.9573\n","0.9655166666666667 0.9611\n","0.96915 0.9638\n","0.9696 0.9645\n","0.97225 0.9664\n","0.9750833333333333 0.968\n","0.9759833333333333 0.9681\n","0.9777166666666667 0.969\n","0.9774 0.9678\n"]}]},{"cell_type":"markdown","source":["# 5.8 정리"],"metadata":{"id":"mjP6FLDJBo1L"}},{"cell_type":"markdown","source":["계산 그래프를 이용하여 신경망의 동작과 오차역전파법을 알아봤다.   \n","이 처리 과정을 계층(layer)이라는 단위로 구현했다.   \n","예를 들어 ReLU 계층, Softmax-with-Loss 계층, Affine 계층, softmax 계층 등이다.   \n","모든 계층에서 forward와 backward 메서드를 구현했다.   \n","전자는 데이터를 순방향으로 전파하고, 후자는 역방향으로 전파함으로써 가중치 매개변수의 기울기를 효율적으로 구할 수 있었다.   \n","\n","이처럼 계산 그래프의 노드들은 국소적인 계산으로 구성된다.   \n","순전파는 통상의 계산을 수행하고, 역전파는 각 노드의 미분을 구한다.   \n","수치 미분과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 잘못이 있는지 없는지 확인할 수 있다."],"metadata":{"id":"Ood0uz0bxtVf"}}]}