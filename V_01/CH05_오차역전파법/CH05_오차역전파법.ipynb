{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CH05_오차역전파법.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOfXwHQG9rR6oZlVAGpiU/b"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 5. 오차역전파법\n","신경망이 가중치 매개변수에 대한 손실 함수의 기울기를 수치 미분을 사용해 구했다.   \n","하지만 수치 미분은 단순하지만 계산 시간이 오래 걸린다는 단점이 있다.   \n","가중치 매개변수의 기울기를 효율적으로 계산하는 오차역전파법을 배워보자."],"metadata":{"id":"ouMwKjxbAPWH"}},{"cell_type":"markdown","source":["# 5.1 계산 그래프\n","계산 과정을 그래프로 나타낸것을 __계산 그래프__라고 한다.   \n","그래프 자료구조는 여러 노드와 엣지로 표현된다.   \n","간단한 예시를 통해 차근차근 알아보자."],"metadata":{"id":"ksD9JHkSAV2b"}},{"cell_type":"markdown","source":["### 5.1.1 계산 그래프로 풀다\n","문제 1. 슈퍼에서 1개에 100원인 사과를 2개 샀습니다. 이때 지불 금액을 구하세요. 소비세 10%가 부과됩니다.   \n","![](https://blog.kakaocdn.net/dn/biHgge/btqIyea02FF/u5P7jDQgSCPa9D3RMPGnOK/img.png)"],"metadata":{"id":"ilT8hUWDAYWi"}},{"cell_type":"markdown","source":["문제 2. 슈퍼에서 사과 2개, 귤 3개를 샀습니다. 사과는 1개에 100원, 귤을 1개에 150원 입니다. 소비세가 10%일 때 지불 금액을 구하세요.   \n","![](https://blog.kakaocdn.net/dn/bpjCkU/btqIwZkLAfb/iY8xeXo1deO4RqobY2QKKk/img.png)"],"metadata":{"id":"-lYp5ASxvoDF"}},{"cell_type":"markdown","source":["위의 예제와 같이 계산 그래프를 이용한 문제는 다음 흐름으로 진행된다.   \n","1. 계산 그래프를 구성한다.\n","2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다.\n","\n","여기서 2번에 해당되는 진행 단계를 순전파라고 한다.   \n","반대로 오른쪽에서 왼쪽으로 진행되는 것을 역전파라고 한다."],"metadata":{"id":"Nbuqm7rav5yt"}},{"cell_type":"markdown","source":["### 5.1.2 국소적 계산\n","계산 그래프는 국소적 계산을 전파함으로써 최종 결과를 얻는다.   \n","여기서 국소적이란 자신과 직접 관계된 작은 범위를 말한다.   \n","전체 계산이 아무리 복잡하더라도 각 단계에서 하는 일은 해당 노드의 국소적 계산이다.   \n","국소적인 계산은 단순하지만, 결과를 전달함으로써 결국 복잡한 계산을 해낼 수 있게된다."],"metadata":{"id":"B8TGm8WlAahF"}},{"cell_type":"markdown","source":["### 5.1.3 왜 계산 그래프로 푸는가?\n","계산 그래프는 국소적 계산으로 전체가 아무리 복잡해도 각 노드에서 단순한 계산에 집중하여 문제를 단순화할 수 있다.   \n","그리고 중간 계산 결과를 모두 보관할 수 있다.   \n","가장 큰 이유는 __역전파를 통해 미분을 효율적으로 계산할 수 있다.__   \n","\n","사과 가격이 오르면 최종 금액에 어떤 영향을 끼치는지 알고 싶을때 어떻게 해야할까?   \n","'사과 가격에 대한 지불 금액의 미분'을 구하는 문제에 해당된다.   \n","사과 값을 $x$, 지불 금액을 $L$이라 했을 때 ${{\\alpha L} \\over{\\alpha x}}$를 구하는 것이다.   \n","이는 사과 값이 아주 조금 올랐을 때 지불 금액이 얼마나 증가하는지 표시한 것이다.   "],"metadata":{"id":"HJ8Hf1UjAb3T"}},{"cell_type":"markdown","source":["![](https://mblogthumb-phinf.pstatic.net/MjAxODA2MjNfMTQw/MDAxNTI5NzM2NTEzMzcz.qZSa133HKdSkflstjt9bxI2znQvhRs1c20sxj0RdX7Qg.DI2D8zxzu3fJ5G5dZ_6YVjKn2OaExmjMAg5dQDKlJFsg.PNG.ssdyka/fig_5-5.png?type=w2)\n","\n","역전파 과정을 통해 미분을 한다.   \n","중간까지 구한 결과를 공유할 수 있어서 다수의 미분을 효율적으로 계산할 수 있다.   \n","\n","계산 그래프의 이점을 다시 한번 정리하자면 __순전파와 역전파를 활용해 각 변수의 미분을 효율적으로 구할 수 있다.__"],"metadata":{"id":"yUE383bDyCau"}},{"cell_type":"markdown","source":["# 5.2 연쇄법칙\n","역전파는 국소적인 미분을 오른쪽에서 왼쪽으로 전달한다.   \n","이를 전달하는 원리는 연쇄법칙에 따른 것이다.   \n","연쇄법칙과 이것이 계산 그래프상의 역전파와 같다는 사실을 알아보자."],"metadata":{"id":"wYGK22xoAeuP"}},{"cell_type":"markdown","source":["### 5.2.1 계산 그래프의 역전파\n","$y=f(x)$ 수식의 역전파를 계산 그래프로 확인해보자.   \n","![](https://blog.kakaocdn.net/dn/FggJa/btqIqmVta4z/hDrKdVzzvHX7wAElrQ0Q4K/img.png)\n","\n","신호 $E$의 노드에 국소적 미분 $({\\alpha y \\over{\\alpha x}})$를 곱한 후 다음 노드로 전달하는 것이다.\n","\n","이러한 방식이 연쇄법칙의 원리로 미분 값을 효율적으로 구할 수 있다는 것이 역전파의 핵심이다.   "],"metadata":{"id":"AvAPUn4hAgWz"}},{"cell_type":"markdown","source":["### 5.2.2 연쇄법칙이란?\n","연쇄법칙을 알기 위해 합성 함수부터 알아야 한다.   \n","__합성 함수__는 여러 함수로 구성된 함수이다.   \n","예를 들어 $z = (x+y)^2$는 $z = t^2$와 $t = x+y$로 두 개의 식으로 구성된다.\n","\n","연쇄법칙은 합성 함수의 미분에 대한 성질이고 다음과 같이 나타낼 수 있다.   \n","> 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.\n","\n","위 식은 ${\\alpha z \\over {\\alpha x}}$ (x에 대한 z의 미분)은 ${\\alpha z \\over {\\alpha t}}$ (t에 대한 z의 미분)과 ${\\alpha t \\over {\\alpha x}}$ (x에 대한 t의 미분)의 곱으로 나타낼 수 있다.   \n","\n","${\\alpha z \\over {\\alpha x}} = {\\alpha z \\over {\\alpha t}}{\\alpha t \\over {\\alpha x}}$   \n","여기서 $\\alpha t$를 지울 수 있다.   \n","${\\alpha z \\over {\\alpha x}} = {\\alpha z \\over {\\alpha x}}$   \n","\n","연쇄법칙을 써서 미분 ${\\alpha z \\over {\\alpha x}}$를 구해보자. 먼저 국소적 미분(편미분)을 구한다.   \n","${\\alpha z \\over {\\alpha t}} = 2t$   \n","${\\alpha t \\over {\\alpha x}} = 1$   \n","\n","미분 공식을 통해 해석적으로 구했는데, 최종적으로 구하고 싶은 ${\\alpha z \\over {\\alpha x}}$는 위에서 구한 두 미분을 곱해서 계산하면 된다.   \n","${\\alpha z \\over {\\alpha x}} = {\\alpha z \\over {\\alpha t}} {\\alpha t \\over {\\alpha x}} = 2t * 1 = 2(x+y)$"],"metadata":{"id":"z_24CnBgAijJ"}},{"cell_type":"markdown","source":["### 5.2.3 연쇄법칙과 계산 그래프\n","![](https://blog.kakaocdn.net/dn/9U9J5/btqIvDCbsVv/mS9ZrimZQoD2Ghzu85HIcK/img.png)\n","\n","노드로 들어온 입력 신호에 그 노드의 국소적 미분을 곱한 후 다음 노드로 전달한다.\n","\n","가장 왼쪽의 역전파를 보면 연쇄법칙이 성립되어 $x$에 대한 $z$의 미분이 된다.   \n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FBLeIk%2FbtqInOkpSrO%2FZriGfBidyOqeudtrwgWwnk%2Fimg.png)   \n","계산 그래프의 역전파 결과에 따르면 ${\\alpha z \\over {\\alpha x}}$는 $2(x+y)$임을 확인할 수 있다."],"metadata":{"id":"kX-UYLBJAjxC"}},{"cell_type":"markdown","source":["# 5.3 역전파\n","덧셈과 곱셈 노드에 따른 역전파를 알아보자."],"metadata":{"id":"nX8nR60RAmpX"}},{"cell_type":"markdown","source":["### 5.3.1 덧셈 노드의 역전파\n","$z = x+y$라는 식을 대상으로 역전파를 살펴보자.   \n","위 식의 미분은 다음과 같이 해석적으로 계산할 수 있다.   \n","${\\alpha z \\over {\\alpha x}} = 1$   \n","${\\alpha z \\over {\\alpha y}} = 1$   \n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FweVPG%2FbtqIwZebgfD%2FbM9nRxwgTxym9kabDhoMaK%2Fimg.png)   \n","역전파에서는 상류에서 전해진 미분에 1을 곱하여 하류로 흘린다.   \n","덧셈 노드의 역전파는 1을 곱하기만 하면 되므로 __입력된 값을 그대로 다음 노드로 보낸다.__   \n","\n","미분 값으로 ${\\alpha L \\over {\\alpha z}}$이 됐는데, 이는 최종적으로 $L$이라는 값을 출력하는 큰 계산 그래프를 가정하기 때문이다.   \n","$z=x+y$ 계산은 큰 계산 그래프의 중간 어딘가에 존재하고, 상류로부터 전해져온 값을 의미한다.\n","\n","구체적인 예로 '10+5=15'라는 계산이 있고, 상류에서 1.3이라는 값이 흘러온다. 이는 다음과 같이 표현된다.   \n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwryMw%2FbtqIs83pNrO%2FbQDbak0dicJeaklazsMGgk%2Fimg.png)"],"metadata":{"id":"buim9yaZAoN7"}},{"cell_type":"markdown","source":["### 5.3.2 곱셈 노드의 역전파\n","$z = xy$라는 식을 생각해보자. 이 식의 미분은 다음과 같다.   \n","${\\alpha z \\over {\\alpha x}} = y$   \n","${\\alpha z \\over {\\alpha y}} = x$   \n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcWoZmR%2FbtqIvXU9pEN%2FIiCVTyc4TrfxSJ7pNgjrr0%2Fimg.png)\n","\n","구체적인 예로 '10x5=50'이라는 계산을 역전파로 해보자.   \n","이때 상류에서 1.3 값이 흘러온다.   \n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbhqAsb%2FbtqInOxU0Uh%2FtoTOhOl8x5IXIvMGfrSlkk%2Fimg.png)\n","\n","덧셈의 역전파는 상류의 값을 그대로 흘려보내서 순방향 입력 신호의 값이 필요하지 않았지만, 곱셈의 역전파는 순방향 입력 신호의 값이 필요하다.   \n","그래서 곱셈 노드를 구현할 때는 순전파의 입력 신호를 변수에 저장한다."],"metadata":{"id":"ODZS7AeSAp0C"}},{"cell_type":"markdown","source":["### 5.3.3 사과 쇼핑의 예\n","![](https://mblogthumb-phinf.pstatic.net/MjAxODA5MDlfMjU0/MDAxNTM2NDY5MTYyNzM0.NYHbHx8H7fvCbcRFH3Hd7fkvqiJ6MnrRe8Q_cFoOD90g.Ds-9h4CYQDxU4Y80m1PQDktedGIh6_nq-gWN-uQiZEkg.PNG.ssdyka/fig_5-17.png?type=w2)"],"metadata":{"id":"HHfej0ULA_UY"}},{"cell_type":"markdown","source":["# 5.4 단순한 계층 구현하기\n","![](https://mblogthumb-phinf.pstatic.net/MjAxODA2MjNfMTQw/MDAxNTI5NzM2NTEzMzcz.qZSa133HKdSkflstjt9bxI2znQvhRs1c20sxj0RdX7Qg.DI2D8zxzu3fJ5G5dZ_6YVjKn2OaExmjMAg5dQDKlJFsg.PNG.ssdyka/fig_5-5.png?type=w2)   \n","사과 쇼핑의 예를 파이썬으로 구현해보자.   \n","계산 그래프의 곱셈 노드를 MulLayer, 덧셈 노드를 AddLayer라는 이름으로 구현해보자."],"metadata":{"id":"xnlcbf_eBBRt"}},{"cell_type":"markdown","source":["### 5.4.1 곱셈 계층\n","곱셈 계층을 구현해보자."],"metadata":{"id":"rK2uS9VnBCSc"}},{"cell_type":"code","source":["class MulLayer:\n","    def __init__(self):\n","        self.x = None\n","        self.y = None\n","    \n","    def forward(self, x, y):\n","        self.x = x\n","        self.y = y\n","        out = x * y\n","        return out\n","    \n","    def backward(self, dout):\n","        dx = dout * self.y\n","        dy = dout * self.x\n","        return dx, dy"],"metadata":{"id":"PbAS8aseK4jU","executionInfo":{"status":"ok","timestamp":1657982030124,"user_tz":-540,"elapsed":369,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["apple = 100\n","apple_num = 2\n","tax = 1.1\n","\n","# layer\n","mul_apple_layer = MulLayer()\n","mul_tax_layer = MulLayer()\n","\n","# 순전파\n","apple_price = mul_apple_layer.forward(apple, apple_num)\n","price = mul_tax_layer.forward(apple_price, tax)\n","\n","print(price) # 220"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"92iSUXgoLS56","executionInfo":{"status":"ok","timestamp":1657982030515,"user_tz":-540,"elapsed":5,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"a10e35fb-e960-4f33-eb1c-aafa49b750b7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["220.00000000000003\n"]}]},{"cell_type":"code","source":["# 역전파\n","dprice = 1\n","dapple_price, dtax = mul_tax_layer.backward(dprice)\n","dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n","\n","print(dapple, dapple_num, dtax)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B6lDOnfuP1B9","executionInfo":{"status":"ok","timestamp":1657982030515,"user_tz":-540,"elapsed":4,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"08636326-608e-465b-9ac2-d9608183c750"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["2.2 110.00000000000001 200\n"]}]},{"cell_type":"markdown","source":["backward()가 받는 인수는 순전파의 출력에 대한 미분임을 주의하자."],"metadata":{"id":"Ns-lX8O_QO_V"}},{"cell_type":"markdown","source":["### 5.4.2 덧셈 계층\n","덧셈 계층을 구현해보자."],"metadata":{"id":"Eoy1uJTtBD5T"}},{"cell_type":"code","source":["class AddLayer:\n","    def __init__(self):\n","        pass\n","    \n","    def forward(self, x, y):\n","        out = x + y\n","        return out\n","    \n","    def backward(self, dout):\n","        dx = dout * 1\n","        dy = dout * 1\n","        return dx, dy"],"metadata":{"id":"n03aaNd0URAu","executionInfo":{"status":"ok","timestamp":1657982030515,"user_tz":-540,"elapsed":3,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["![](https://mblogthumb-phinf.pstatic.net/MjAxODA5MDlfMjU0/MDAxNTM2NDY5MTYyNzM0.NYHbHx8H7fvCbcRFH3Hd7fkvqiJ6MnrRe8Q_cFoOD90g.Ds-9h4CYQDxU4Y80m1PQDktedGIh6_nq-gWN-uQiZEkg.PNG.ssdyka/fig_5-17.png?type=w2)\n","\n","위 이미지 상황을 구현해보자."],"metadata":{"id":"WZkRqMpvUh22"}},{"cell_type":"code","source":["apple = 100\n","apple_num = 2\n","orange = 150\n","orange_num = 3\n","tax = 1.1\n","\n","# layer\n","mul_apple_layer = MulLayer()\n","mul_orange_layer = MulLayer()\n","add_apple_orange_layer = AddLayer()\n","mul_tax_layer = MulLayer()\n","\n","# forward\n","apple_price = mul_apple_layer.forward(apple, apple_num)\n","orange_price = mul_orange_layer.forward(orange, orange_num)\n","all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n","price = mul_tax_layer.forward(all_price, tax)\n","\n","# backward\n","dprice = 1\n","dall_price, dtax = mul_tax_layer.backward(dprice)\n","dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n","dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n","dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n","\n","print(price)\n","print(dapple_num, dapple, dorange, dorange_num, dtax)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUakRRnmUjiW","executionInfo":{"status":"ok","timestamp":1657982030515,"user_tz":-540,"elapsed":3,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"15141517-8f38-44b3-843f-05346283b99b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["715.0000000000001\n","110.00000000000001 2.2 3.3000000000000003 165.0 650\n"]}]},{"cell_type":"markdown","source":["# 5.5 활성화 함수 계층 구현하기\n","계산 그래프를 신경망에 적용해보자.   \n","신경망을 구성하는 layer 각각을 클래스 하나로 구현하자.   \n","먼저 활성화 함수인 ReLU와 Sigmoid를 구현해보자."],"metadata":{"id":"-2ONX-fVBF0J"}},{"cell_type":"markdown","source":["### 5.5.1 ReLU 계층\n","![](https://blog.kakaocdn.net/dn/vgJna/btqQzRGmwcO/TK3KTMlz4CYag8rBTKfYkK/img.png)\n","\n","순전파에서 $x$가 0보다 크면 역전파는 상류의 값을 그대로 하류로 흘린다.   \n","$x$가 0보다 작으면 하류로 0을 보낸다."],"metadata":{"id":"jje2ul9pBHNL"}},{"cell_type":"code","source":["class ReLU:\n","    def __init__(self):\n","        self.mask = None\n","    \n","    def forward(self, x):\n","        self.mask = (x <= 0)\n","        out = x.copy()\n","        out[self.mask] = 0\n","        return out\n","    \n","    def backward(self, dout):\n","        dout[self.mask] = 0\n","        dx = dout\n","        return dx"],"metadata":{"id":"xfMwc3DDWsJ4","executionInfo":{"status":"ok","timestamp":1657982030515,"user_tz":-540,"elapsed":3,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["mask라는 인스턴스 변수를 가진다.   \n","이는 True, False로 구성된 넘파이 배열로 순전파 입력인 x의 원소 값이 0 이하인 인덱스는 True, 그외는 False로 유지한다.\n","\n","역전파는 순전판때 만들어둔 mask를 써서, mask의 원소가 True일 경우에 상류에서 전파된 dout을 0으로 설정한다."],"metadata":{"id":"F5P41T3aW8ST"}},{"cell_type":"code","source":["import numpy as np\n","x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n","print(x)\n","mask = (x <= 0)\n","print(mask)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"byiE0cXyW7fr","executionInfo":{"status":"ok","timestamp":1657982030515,"user_tz":-540,"elapsed":2,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"5a713d20-02a5-4211-c8c9-f04cb7ca1287"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 1.  -0.5]\n"," [-2.   3. ]]\n","[[False  True]\n"," [ True False]]\n"]}]},{"cell_type":"markdown","source":["### 5.5.2 Sigmoid 계층\n","![](https://t1.daumcdn.net/cfile/tistory/275BAD4F577B669920)\n","\n"],"metadata":{"id":"1soJjurUBJSo"}},{"cell_type":"markdown","source":["시그모이드 함수는 다음 식을 의미한다.   \n","$y = {1\\over{1+exp(-x)}}$\n","\n","시그모이드 계층의 계산 그래프는 '*', '+', 'exp', '/' 노드가 등장한다.   \n","'exp' 노드는 $y = exp(x)$ 계산을 수행하고 '/' 노드는 $y = {1\\over{x}}$ 계산을 수행한다.\n","\n","![](https://velog.velcdn.com/images%2Flilpark%2Fpost%2Fe4fc1d53-2443-477e-a9b0-3e5a9fae8eb3%2Fimage.png)\n","\n","이를 계산 그래프에서 간소화하면 다음과 같이 나온다.   \n","![](https://velog.velcdn.com/images%2Flilpark%2Fpost%2F2b877b61-b267-4852-9022-a9da55c1db00%2Fimage.png)"],"metadata":{"id":"ZrRqoopJ92SM"}},{"cell_type":"markdown","source":["${\\alpha L \\over {\\alpha y}}y^2 exp(-x)$   \n","$= {\\alpha L \\over {\\alpha y}}{1 \\over {(1+exp(-x))^2}}exp(-x)$   \n","$= {\\alpha L \\over {\\alpha y}}{1 \\over {1 + exp(-x)}} {exp(-x) \\over 1+exp(-x)}$   \n","$= {\\alpha L \\over {\\alpha y}}y(1-y)$\n","\n","위 계산식을 보면 sigmoid 계층의 역전파는 순전파의 출력 $y$의 값만으로 계산을 진행할 수 있게 된다. 그래서 다음과 같이 표현할 수 있게 된다.   \n","\n","![](https://blog.kakaocdn.net/dn/bR5ka7/btrbN5cJFxP/u0hZweWlmhIHq6oGovvK51/img.png)"],"metadata":{"id":"BgLLtiRZ_B52"}},{"cell_type":"code","source":["class Sigmoid:\n","    def __init__(self):\n","        self.out = None\n","    \n","    def forward(self, x):\n","        out = 1 / (1 + np.exp(-x))\n","        self.out = out\n","\n","        return out\n","    \n","    def backward(self, dout):\n","        dx = dout * (1.0 - self.out) * self.out\n","\n","        return dx"],"metadata":{"id":"xwiCoWqlAvjd","executionInfo":{"status":"ok","timestamp":1657982030515,"user_tz":-540,"elapsed":2,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["위 구현에서는 순전파의 출력을 out 변수에 보관했다가, 역전파 계산할 때 그 값을 사용한다."],"metadata":{"id":"pNSTcratA-jw"}},{"cell_type":"markdown","source":["# 5.6 Affine/Softmax 계층 구현하기"],"metadata":{"id":"I_YnWXKdBM3f"}},{"cell_type":"markdown","source":["### 5.6.1 Affine 계층\n","신경망의 순전파에서 가중치 신호의 총합을 계산하기 위해 행렬의 곱(np.dot())을 사용했다.   \n","예시를 확인해보자."],"metadata":{"id":"-A5-BWRVBWTu"}},{"cell_type":"code","source":["X = np.random.rand(2) # 입력\n","W = np.random.rand(2,3) # weight\n","B = np.random.rand(3) # bias\n","\n","print(X.shape)\n","print(W.shape)\n","print(B.shape)\n","\n","Y = np.dot(X, W) + B\n","print(Y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n81i5SPnB59E","executionInfo":{"status":"ok","timestamp":1657982030935,"user_tz":-540,"elapsed":13,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"dd4f5141-4cc1-41ca-cff4-9a46aaba8cd5"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["(2,)\n","(2, 3)\n","(3,)\n","(3,)\n"]}]},{"cell_type":"markdown","source":["행렬곱의 핵심은 대응하는 차원의 원소 수를 일치시키는 게 핵심이다.   \n","\n","> 신경망의 순전파 때 수행하는 행렬의 곱을 기하학에서 __어파인 변환(affine transformation)__ 이라고 한다.   \n","그리고 이것을 Affine 계층이라는 이름으로 구현해보자.\n","\n","지금까지 계산 그래프의 노드에 스칼라값이 있었는데 이번엔 행렬이 흐르고 있다.   \n","여기에 역전파에 대해 생각해보자.   \n","행렬을 사용한 역전파도 행렬의 원소마다 전개해보면 스칼라값을 사용한 지금까지의 계산 그래프와 같은 순서로 생각할 수 있다.   \n","\n","${\\alpha L \\over {\\alpha X}} = {\\alpha L \\over {\\alpha Y}} W^T$   \n","${\\alpha L \\over {\\alpha L}} = X^t {\\alpha L \\over {\\alpha Y}}$   \n","\n","여기서 $T$는 전치행렬을 뜻한다.   \n","이는 $W$의 $(i, j)$ 위치의 원소를 $(j, i)$ 위치로 바꾼 것을 말한다.   \n","\n","![](https://blog.kakaocdn.net/dn/bKEIN4/btqIs83DdjS/XITy2ucBrqdnn8prfmiNZK/img.png)   \n","\n","스칼라값이 아닌 행렬 곱을 진행하기 위해 행렬의 형상에 주의해야한다.   "],"metadata":{"id":"bj68xRe1Cd-S"}},{"cell_type":"markdown","source":["### 5.6.2 배치용 Affine 계층\n","데이터 N개를 묶어서 순전파하는 경우인 배치용 Affine 계층을 생각해보자.   \n","\n","![](https://blog.kakaocdn.net/dn/cTcDvH/btqIxY66Rej/LGESmiYoG0tY32Ls2KyNvK/img.png)\n","\n","입력인 $X$의 형상이 (N,2)가 됐다.   \n","\n","편향을 더할 때 주의해야한다.   \n","순전파의 편향 덧셈은 $XW$에 대한 평향이 각 데이터에 더해진다."],"metadata":{"id":"rj59qEUIBZIq"}},{"cell_type":"code","source":["X_dot_W = np.array([[0,0,0], [10,10,10]])\n","B = np.array([1,2,3])\n","\n","print(X_dot_W)\n","\n","print(X_dot_W + B)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jpOm-XekJpH5","executionInfo":{"status":"ok","timestamp":1657982030936,"user_tz":-540,"elapsed":11,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"0ec2139e-db2f-4f90-8de2-0de398a4c2c6"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0  0  0]\n"," [10 10 10]]\n","[[ 1  2  3]\n"," [11 12 13]]\n"]}]},{"cell_type":"markdown","source":["순전파에서 편향 덧셈은 각각의 데이터에 더해진다.   \n","그래서 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 한다."],"metadata":{"id":"W3TQWoHTJzr2"}},{"cell_type":"code","source":["dY = np.array([[1,2,3], [4,5,6]])\n","print(dY)\n","\n","dB = np.sum(dY, axis=0)\n","print(dB)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxNtw1uOJzIp","executionInfo":{"status":"ok","timestamp":1657982030936,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"ed340fa4-5a68-4298-ed48-c10895ef6ef4"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1 2 3]\n"," [4 5 6]]\n","[5 7 9]\n"]}]},{"cell_type":"markdown","source":["이 예에서 데이터가 2개라고 가정했다.   \n","편향의 역전파는 두 데이터에 대한 미분을 데이터마다 더해서 구한다.   \n","그래서 np.sum()에서 0번째축에 대해 총합을 구하는 것이다.   \n","\n","Affine 구현을 확인해보자."],"metadata":{"id":"w4B3OB8TKEcF"}},{"cell_type":"code","source":["class Affine:\n","    def __init__(self, W, b):\n","        self.W = W\n","        self.b = b\n","        self.x = None\n","        self.dW = None\n","        self.db = None\n","\n","    def forward(self, x):\n","        self.x = x\n","        out = np.dot(x, self.W) + self.b\n","\n","        return out\n","    \n","    def backward(self, dout):\n","        dx = np.dot(dout, self.W.T)\n","        self.dW = np.dot(self.x.T, dout)\n","        self.db = np.sum(dout, axis=0)\n","\n","        return dx"],"metadata":{"id":"aMOUK3dD1pDc","executionInfo":{"status":"ok","timestamp":1657982030936,"user_tz":-540,"elapsed":5,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### 5.6.3 Softmax-with-Loss 계층\n","출력층에서 사용하는 소프트맥스 함수에 대해 알아보자.   \n","소프트맥스 함수는 입력 값을 정규화하여 출력한다.   \n","\n","손글씨 숫자 인식에서 다음과 같은 순서로 진행한다.   \n","> 입력 이미지가 Affine 계층과 ReLU 계층을 통과하며 변환하고, 마지막 Softmax 계층에서 10개(숫자 0~9)의 입력이 정규화된다.   \n","최종적으로 Softmax 결과의 값이 확률이 된다.\n","\n","> 신경망은 학습과 추론 작업으로 나뉜다.   \n","추론은 일반적으로 Softmax를 사용하지 않는다.   \n","추론할 때는 마지막 Affine 계층의 출력을 인식 결과로 이용한다.   \n","신경망에서 정규화하지 않는 출력 결과를 __점수(score)__라고 한다.   \n","신경망 추론에서 답을 하나만 내는 경우에는 가장 높은 저무만 알면 되니 Softmax는 필요 없어진다.\n","\n","소프트맥스 레이어를 구현하기 위해 손실 함수의 교차 엔트로피 오차도 포함하여 구현해보자.   \n","먼저 softmax-with-loss 계층의 계산 그래프를 살펴보자.   \n","![](https://i.imgur.com/UPTYfXt.png)"],"metadata":{"id":"M8SXZX-WBbLW"}},{"cell_type":"markdown","source":["그리고 아래와 같이 간소화 할 수 있다.   \n","![](https://velog.velcdn.com/images/bbirong/post/018ee92a-af64-4239-b277-7ddf3add9e16/image.png)"],"metadata":{"id":"Koan1ijXX5MY"}},{"cell_type":"markdown","source":["3개의 클래스 분류를 가정하고 이전 계층에서 3개의 입력(점수)를 받는다.   \n","Softmax 계층은 입력 $(a_1, a_2, a_3)$을 정규화하여 $(y_1, y_2, y_3)$을 출력한다.   \n","Cross Entropy Error 계층은 Softmax의 출력과 정답 테이블 $(t_1, t_2, t_3)$을 받고, 이 데이터들로부터 손실 $L$을 출력한다.   \n","\n","역전파의 결과를 보면 Softmax 계층은 $(y_1-t_1, y_2-t_2, y_3-t_3)$이라는 말끔한 결과를 얻는다.   \n","이는 Softmax 계층의 출력과 정답 레이블의 차분이다.   \n","신경망의 역전파에선 오차가 앞 계층에 전해진다.   \n","__이는 신경망 학습의 중요한 성질이다.__\n","\n","신경망 학습의 목적은 신경망의 출력이 정답 레이블과 가까워지도록 가중치 매개변수의 값을 조정하는 것이다.   \n","그래서 신경망의 출력과 정답 레이블의 오차를 효율적으로 앞 계층에 전달해야한다.   \n","그래서 역전파를 통해 얻는 결과는 Softmax 계층의 출력과 정답 레이블의 차이로, 신경망의 현재 출력과 정답 레이블의 오차를 있는 그대로 드러내는 것이다.   \n","\n","> Softmax 함수의 손실 함수로 Cross Entropy Error를 사용하니 역전파가 말끔히 떨어진다.   \n","이는 우연이 아닌 그렇게 설계되었기 때문이다.   \n","회귀의 출력층에서 사용하는 항등 함수의 손실 함수로 오차제곱합을 이용하는 이유도 이와 동일하다.\n","\n","구체적인 예를 들어보자.   \n","정답 레이블이 (0, 1, 0)이고 Softmax가 (0.3, 0.2, 0.5)를 출력했다.   \n","정답 인덱스에 맞는 확률이 20%로 나와 신경망은 제대로 인식하지 못함을 말한다.   \n","이 경우 Softmax의 역전파는 (0.3, -0.8, 0.5)라는 커다란 오차를 전파한다.   \n","그래서 Softmax의 앞 계층들은 큰 오차로부터 큰 깨달음을 얻게 된다.   \n","\n","정답 레이블이 (0, 1, 0), Softmax가 (0.01, 0.99, 0)일 때를 가정해보자.   \n","이 경우 Softmax 계층의 역전파가 보내는 오차는 비교적 작은 (0.01, -0.01, 0)이 된다.   \n","앞 계층으로 전달된 오차가 작으므로 학습하는 정도도 작아진다.\n","\n","Softmax-with_Loss 계층을 구현한 코드를 확인해보자."],"metadata":{"id":"-weR0xU6YKUb"}},{"cell_type":"code","source":["def softmax(x):\n","    x = x - np.max(x, axis=-1, keepdims=True)\n","    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n","\n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","        \n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","             \n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"],"metadata":{"id":"cuzwFXZXbx_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SoftmaxWithLoss:\n","    def __init__(self):\n","        self.loss = None # 손실\n","        self.y = None # Softmax 출력\n","        self.t = None # 정답 레이블\n","\n","    def forward(self, x, t):\n","        self.t = t\n","        self.y = softmax(x)\n","        self.loss = cross_entropy_error(self.y, self.t)\n","        return self.loss\n","\n","    def backward(self, dout=1):\n","        batch_size = self.t.shape[0]\n","        dx = (self.y - self.t) / batch_size\n","        return dx"],"metadata":{"id":"PvWqOIFebANs","executionInfo":{"status":"ok","timestamp":1657982030937,"user_tz":-540,"elapsed":6,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["역전파 때는 전파하는 값을 배치의 수로 나눠서 데이터 1개당 오차를 앞 계층으로 전파하는 점을 주의하자."],"metadata":{"id":"iLrA_-wbbhI2"}},{"cell_type":"markdown","source":["# 5.7 오차역전파법 구현하기"],"metadata":{"id":"siyO-osRBe0i"}},{"cell_type":"markdown","source":["### 5.7.1 신경망 학습의 전체 그림"],"metadata":{"id":"SJaaMwjKBglB"}},{"cell_type":"markdown","source":["### 5.7.2 오차역전파법을 적용한 신경망 구현하기"],"metadata":{"id":"OJyjQicqBiAM"}},{"cell_type":"markdown","source":["### 5.7.3 오차역전파법으로 구한 기울기 검증하기"],"metadata":{"id":"f6bAQEMABk9q"}},{"cell_type":"markdown","source":["### 5.7.4 오차역전바법을 사용한 학습 구현하기"],"metadata":{"id":"igrQBbjTBnB-"}},{"cell_type":"markdown","source":["# 5.8 정리"],"metadata":{"id":"mjP6FLDJBo1L"}}]}