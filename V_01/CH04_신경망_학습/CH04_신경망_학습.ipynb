{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CH04_신경망_학습.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1cYwKIMQcfF-cNcF73ABnroPoU6udSHtY","authorship_tag":"ABX9TyOEQeJRRNJnMmhMhSBW7dL5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 4. 신경망 학습\n","학습을 통해 가중치 매개변수의 최적값을 자동으로 획득해보자.   \n","신경망을 학습할 수 있게 해주는 지표인 손실 함수를 통해, 손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 학습의 목표이다.   \n","함수의 기울기를 활용하는 경사법을 알아보자."],"metadata":{"id":"unGyDlE-Hv5y"}},{"cell_type":"markdown","source":["# 4.1 데이터에서 학습한다!\n","신경망은 데이터를 보고 가중치 매개변수의 값을 자동으로 결정한다.   \n","신경망 학습과 MNIST 데이터셋의 손글씨 숫자를 학습하는걸 코드로 구현해보자."],"metadata":{"id":"k_f6hdXZHycq"}},{"cell_type":"markdown","source":["### 4.1.1 데이터 주도 학습\n","머신러닝은 데이터가 생명이다.   \n","데이터에서 패턴을 발견해 답을 찾는다.\n","\n","보통 문제를 해결하기 위해, 사람이 생각하고 답을 찾는게 일반적이다.   \n","그리고 사람의 경험과 직관을 단서로 시행착오를 거듭하며 일을 진행한다.   \n","하지만 기계학습은 사람의 개입을 최소화하고 데이터로부터 패턴을 찾으려 시도한다.   \n","특히 신경망과 딥러닝은 기존 기계학습보다 사람의 개입을 더욱 배제한다.\n","\n","사람이 숫자를 분류하는 프로그램을 설계하려면, 그 안에 숨은 규칙성을 명확한 로직으로 풀기 어렵다.   \n","그래서 신경망은 이미지에서 __특징(feature)__을 추출하고 특징의 패턴을 기계학습 기술로 학습한다.   \n","여기서 특징은 입력 데이터에서 중요한 데이터를 정확하게 추출할 수 있도록 설계된 변환기를 가리킨다.\n","\n","기계학습은 모아진 데이터로부터 기계가 규칙을 찾아낸다.   \n","하지만 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 사람이 설계해야 한다.   \n","이는 문제에 적합한 특징을 사용하지 않으면 좋은 결과를 얻을 수 없음을 뜻한다.   \n","즉 기계학습도 사람이 적절한 특징을 생각해야한다.   \n","\n","신경망은 이미지에 포함된 중요한 특징까지 기계가 스스로 학습하게 된다.   \n","신경망의 이점으로 모든 문제를 같은 맥락에서 풀 수 있고, 주어진 문제의 패턴을 발견하려 시도한다.   \n","즉, 모든 문제를 주어딘 데이터 그대로를 입력으로 활용해 end-to-end로 학습할 수 있다."],"metadata":{"id":"D2vJhjBuIgs7"}},{"cell_type":"markdown","source":["### 4.1.2 훈련 데이터와 시험 데이터\n","기계학습은 범용 능력(아직 보지 못한 데이터)을 제대로 획득하는 것이 최종 목표이다.   \n","그래서 데이터셋을 훈련 데이터와 시험 데이터를 나눠서 평가한다.   \n","하지만 데이터셋 하나로만 학습과 평가를 진행하면 수중의 데이터셋만 지나치게 최적화된 오버피팅 현상이 나올 수도 있다."],"metadata":{"id":"d3AsFtZHQV6G"}},{"cell_type":"markdown","source":["# 4.2 손실 함수\n","신경망에서 현재의 상태를 하나의 지표로 표현한다.   \n","그리고 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색해야한다.   \n","신경망 학습에서 사용하는 지표는 __손실 함수(loss function)__이다.   \n","일반적으로 오차제곱합, 교체 엔트로피 오차를 사용한다."],"metadata":{"id":"-7G5Mn3oTBQp"}},{"cell_type":"markdown","source":["### 4.2.1 오차제곱합\n","__오차제곱합(sum of squares for error, SSE)__을 알아보자.   \n","$E = {{1}\\over{2}}\\sum\\limits_k(y_k-t_k)^2$   \n","$y_k$는 신경망의 출력, $t_k$는 정답 레이블, $k$는 데이터의 차원 수를 나타낸다."],"metadata":{"id":"oLJ0L674VPTW"}},{"cell_type":"markdown","source":["여기서 y는 소프트맥스의 출력으로 확률로 해석하자.   \n","그리고 t는 정답만 1로 나타내는 원-핫 인코딩 형태이다."],"metadata":{"id":"1u0tV6NNW0kJ"}},{"cell_type":"code","source":["import numpy as np\n","\n","def sum_squares_error(y, t):\n","    return 0.5 * np.sum((y-t)**2)"],"metadata":{"id":"2yXpPTBLXDQv","executionInfo":{"status":"ok","timestamp":1656945141359,"user_tz":-540,"elapsed":437,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"TmzHqBJxBpcu","executionInfo":{"status":"ok","timestamp":1656945141839,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"outputs":[],"source":["y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"]},{"cell_type":"code","source":["sum_squares_error(np.array(y), np.array(t))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmednmPDXM3Q","executionInfo":{"status":"ok","timestamp":1656945141840,"user_tz":-540,"elapsed":8,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"b3e0126c-d695-4e43-b2aa-f3b6084b9ccf"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.09750000000000003"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","\n","sum_squares_error(np.array(y), np.array(t))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qNwr5CMeXskj","executionInfo":{"status":"ok","timestamp":1656945141840,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"42202805-3ae4-4233-fbae-04e63e6d7f0b"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5975"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["신경망이 추정한 값과 정답이 일치한 상황과 그렇지 않은 상황에 대해 SSE 값을 출력해봤다."],"metadata":{"id":"KQodP_2kX35J"}},{"cell_type":"markdown","source":["### 4.2.2 교차 엔트로피 오차\n","손실 함수로 __교차 엔트로피 오차(cross entropy error, CEE)__를 많이 사용하므로 확인해보자.\n","\n","$E = -\\sum\\limits_kt_klogy_k$\n","\n","여기서 log는 밑이 $e$인 자연로그($log_e$)이다.   \n","$y_k$는 신경망의 출력(추정값), $t_k$는 정답 레이블이고 원-핫 인코딩이다.   \n","그래서 실질적으로 정답일 때 추정($t_k$가 1일때 $y_k$)의 자연로그를 계산하게 된다.   \n","예를들면 정답 레이블에 맞는 신경망의 출력 $y_k$가 0.6이면 교차 엔트로피 오차는 -log0.6 = 0.51이 된다.   \n","같은 조건에서 신경망 출력이 0.1이면 -log0.1 = 2.30이 된다.   \n","\n","![](https://support.minitab.com/ko-kr/minitab/18/naturallog_scatterplot.png)   \n","자연로그의 그래프를 확인해보면 x가 1일때 y가 0이되고, x가 0에 가까워질수록 y의 값은 점점 작아진다."],"metadata":{"id":"brx5ply5YGsM"}},{"cell_type":"code","source":["def cross_entropy_error(y, t):\n","    delta = 1e-7\n","    return -np.sum(t * np.log(y + delta))"],"metadata":{"id":"cfY2h4YqaUSJ","executionInfo":{"status":"ok","timestamp":1656945141840,"user_tz":-540,"elapsed":6,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["np.log() 함수에 0을 넣으면 마이너스 무한대인 -inf가 나와 계산을 진행할 수 없기에, 아주 작은 값인 delta를 넣어줬다."],"metadata":{"id":"nS2rV6Q6adRY"}},{"cell_type":"code","source":["y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","\n","cross_entropy_error(np.array(y), np.array(t))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UiyQ7eGjac4v","executionInfo":{"status":"ok","timestamp":1656945141840,"user_tz":-540,"elapsed":5,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"546a1c55-7915-4e9c-854b-a951b57dc4d9"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.510825457099338"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","\n","cross_entropy_error(np.array(y), np.array(t))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oIA4tHmPgje9","executionInfo":{"status":"ok","timestamp":1656945141840,"user_tz":-540,"elapsed":5,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"33ea0480-cc5d-47dd-df44-3f108470b3d8"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.302584092994546"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["### 4.2.3 미니배치 학습\n","기계학습 문제는 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다.   \n","즉, 훈련 데이터 개수만큼 이들의 손실 함수 값들의 합을 지표로 삼는다.   \n","\n","\b훈련 데이터 모두에 대한 손실 함수의 합을 구하는 방법을 알아보자.   \n","$E = -{{1}\\over{N}}\\sum\\limits_n\\sum\\limits_kt_{nk}logy_{nk}$ \n","\n","데이터가 $N$개이고 $t_{nk}$는 $n$번째 데이터의 $k$번째 값을 의미한다.   \n","그리고 $N$으로 나누어 정규화를 하고, $N$으로 나눔으로써 __평균 손실 함수__를 구하는 것이다.\n","\n","MNIST의 데이터셋은 훈련 데이터가 60000개였다.   \n","모든 데이터를 대상으로 손실 함수의 합을 구하려면 시간이 걸린다.   \n","그래서 데이터의 일부를 추려 전체의 근사치로 이용할 수 있다.   \n","이 일부를 __미니 배치__라고 한다.   \n","즉, 6만장의 훈련 데이터 중에서 100장을 무작위로 뽑아 학습하는 것이다."],"metadata":{"id":"vQgPW8tcgz1V"}},{"cell_type":"markdown","source":["### MNIST dataset 불러오기"],"metadata":{"id":"xnselBSBvBlx"}},{"cell_type":"code","source":["# coding: utf-8\n","try:\n","    import urllib.request\n","except ImportError:\n","    raise ImportError('You should use Python 3.x')\n","import os.path\n","import gzip\n","import pickle\n","import os\n","import numpy as np\n","\n","\n","url_base = 'http://yann.lecun.com/exdb/mnist/'\n","key_file = {\n","    'train_img':'train-images-idx3-ubyte.gz',\n","    'train_label':'train-labels-idx1-ubyte.gz',\n","    'test_img':'t10k-images-idx3-ubyte.gz',\n","    'test_label':'t10k-labels-idx1-ubyte.gz'\n","}\n","\n","dataset_dir = os.path.dirname('/content/drive/Othercomputers/MacBook_Air/Deep_Learning_from_Scratch/V_01/dataset')\n","save_file = dataset_dir + \"/mnist.pkl\"\n","\n","train_num = 60000\n","test_num = 10000\n","img_dim = (1, 28, 28)\n","img_size = 784\n","\n","\n","def _download(file_name):\n","    file_path = dataset_dir + \"/\" + file_name\n","\n","    if os.path.exists(file_path):\n","        return\n","\n","    print(\"Downloading \" + file_name + \" ... \")\n","    headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\"}\n","    request = urllib.request.Request(url_base+file_name, headers=headers)\n","    response = urllib.request.urlopen(request).read()\n","    with open(file_path, mode='wb') as f:\n","        f.write(response)\n","    print(\"Done\")\n","\n","def download_mnist():\n","    for v in key_file.values():\n","       _download(v)\n","\n","def _load_label(file_name):\n","    file_path = dataset_dir + \"/\" + file_name\n","\n","    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n","    with gzip.open(file_path, 'rb') as f:\n","            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n","    print(\"Done\")\n","\n","    return labels\n","\n","def _load_img(file_name):\n","    file_path = dataset_dir + \"/\" + file_name\n","\n","    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n","    with gzip.open(file_path, 'rb') as f:\n","            data = np.frombuffer(f.read(), np.uint8, offset=16)\n","    data = data.reshape(-1, img_size)\n","    print(\"Done\")\n","\n","    return data\n","\n","def _convert_numpy():\n","    dataset = {}\n","    dataset['train_img'] =  _load_img(key_file['train_img'])\n","    dataset['train_label'] = _load_label(key_file['train_label'])\n","    dataset['test_img'] = _load_img(key_file['test_img'])\n","    dataset['test_label'] = _load_label(key_file['test_label'])\n","\n","    return dataset\n","\n","def init_mnist():\n","    download_mnist()\n","    dataset = _convert_numpy()\n","    print(\"Creating pickle file ...\")\n","    with open(save_file, 'wb') as f:\n","        pickle.dump(dataset, f, -1)\n","    print(\"Done!\")\n","\n","def _change_one_hot_label(X):\n","    T = np.zeros((X.size, 10))\n","    for idx, row in enumerate(T):\n","        row[X[idx]] = 1\n","\n","    return T\n","\n","\n","def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n","    \"\"\"MNISTデータセットの読み込み\n","\n","    Parameters\n","    ----------\n","    normalize : 画像のピクセル値を0.0~1.0に正規化する\n","    one_hot_label :\n","        one_hot_labelがTrueの場合、ラベルはone-hot配列として返す\n","        one-hot配列とは、たとえば[0,0,1,0,0,0,0,0,0,0]のような配列\n","    flatten : 画像を一次元配列に平にするかどうか\n","\n","    Returns\n","    -------\n","    (訓練画像, 訓練ラベル), (テスト画像, テストラベル)\n","    \"\"\"\n","    if not os.path.exists(save_file):\n","        init_mnist()\n","\n","    with open(save_file, 'rb') as f:\n","        dataset = pickle.load(f)\n","\n","    if normalize:\n","        for key in ('train_img', 'test_img'):\n","            dataset[key] = dataset[key].astype(np.float32)\n","            dataset[key] /= 255.0\n","\n","    if one_hot_label:\n","        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n","        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n","\n","    if not flatten:\n","         for key in ('train_img', 'test_img'):\n","            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n","\n","    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label'])\n","\n","\n","if __name__ == '__main__':\n","    init_mnist()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E0s3khiSu_U3","executionInfo":{"status":"ok","timestamp":1656945146459,"user_tz":-540,"elapsed":4624,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"a0b9b173-063c-4d52-90f2-4784ef6546ae"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Converting train-images-idx3-ubyte.gz to NumPy Array ...\n","Done\n","Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n","Done\n","Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n","Done\n","Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n","Done\n","Creating pickle file ...\n","Done!\n"]}]},{"cell_type":"code","source":["import sys, os\n","# sys.path.append(os.pardir)\n","import numpy as np\n","# from dataset.mnist import load_mnist\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","print(x_train.shape)\n","print(t_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7c0kH0iTgrF_","executionInfo":{"status":"ok","timestamp":1656945147087,"user_tz":-540,"elapsed":632,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"4ff81090-32b4-442b-ecd5-7fb79d9f6ac6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 784)\n","(60000, 10)\n"]}]},{"cell_type":"code","source":["train_size = x_train.shape[0]\n","batch_size = 10\n","batch_mask = np.random.choice(train_size, batch_size)\n","x_batch = x_train[batch_mask]\n","t_batch = t_train[batch_mask]"],"metadata":{"id":"t7Ek6LTyuGkD","executionInfo":{"status":"ok","timestamp":1656946926201,"user_tz":-540,"elapsed":450,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["np.random.choice(60000, 10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1RN4uG-u6gr","executionInfo":{"status":"ok","timestamp":1656947333824,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"0a0f564d-bf9a-455d-f7f9-abe0fdd2a9eb"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([25501, 56702, 18346, 33430, 51380, 43700,  8164, 10024, 57099,\n","       10590])"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["훈련 데이터에서 무작위로 batch_size만큼 꺼내올 수 있다.   \n","np.random.choice()로 랜덤으로 선택해 인덱스로 사용하여 미니배치를 뽑아내면 된다."],"metadata":{"id":"npTw_C2Du7I2"}},{"cell_type":"markdown","source":["### 4.2.4 (배치용) 교차 엔트로피 오차 구현하기\n","교차 엔트로피 오차에서 배치 데이터를 구현할 수 있게 해보자.   \n","예측한 y의 값이 1차원인 데이터 하나당 교차 엔트로피 오차를 구하는 경우에는 reshape로 데이터의 형상을 바꿔준다.   \n","그리고 배치 크기로 나눠 정규화하고 이미지 1장당 평균의 교차 엔트로피 오차를 계산한다."],"metadata":{"id":"_CA1XFwlwr-r"}},{"cell_type":"code","source":["def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","\n","    batch_size = y.shape[0]\n","    return -np.sum(t * np.log(y + 1e-7)) / batch_size"],"metadata":{"id":"x4zkM_QMxQTV","executionInfo":{"status":"ok","timestamp":1656947904993,"user_tz":-540,"elapsed":440,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["정답이 원-핫 인코딩이 아닌 숫자 레이블로 주어졌을 때 교차 엔트로피 오차를 어떻게 구현할까?"],"metadata":{"id":"3FyTDvNsx5T4"}},{"cell_type":"code","source":["def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","    \n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"],"metadata":{"id":"UMq62dmhyut1","executionInfo":{"status":"ok","timestamp":1656948137685,"user_tz":-540,"elapsed":428,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["이 구현에서 원-핫 인코딩일 때 t가 0인 원소는 교차 엔트로피 오차도 0이므로, 그 계산은 무시해도 좋다는 것이 핵심이다.   \n","이는 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산할 수 있다.   \n","그래서 원-핫 인코딩시 `t*np.log(y)`를 레이블 표현은 `np.log(y[np.arange(batch_size), t])`로 구현한다.\n","\n","`np.log(y[np.arange(batch_size), t])`에서 np.arange(batch_size)는 0부터 batch_size-1까지 배열을 생성한다.   \n","t에는 레이블이 [2,7,0,9,4]와 같이 저장되어 있으므로 `y[np.arange(batch_size], t`는 각 데이터의 정답 레이블에 해당하는 신경망의 출력을 추출한다.   \n","(예를들자면 `[y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]`인 넘파이 배열을 생성한다.)"],"metadata":{"id":"VFJJRFCnzh_y"}},{"cell_type":"markdown","source":["### 4.2.4 왜 손실 함수를 설정하는가?\n","궁극적인 목표는 높은 정확도를 끌어내는 매개변수 값을 찾는 것이다.   \n","그렇다면 왜 정확도 지표를 두고 손실 함수의 값을 택할까?\n","\n","신경망 학습에서 __미분__의 역할이 있기 때문이다.   \n","신경망 학습에서 최적의 매개변수(가중치, 편향)을 탐색할 때 손실 함수의 값을 가능한 작게 하는 매개변수 값을 찾는다.   \n","이때 매개변수의 기울기를 미분을 통해 계산하고, 그 미분 값을 단서로 매개변수의 값을 갱신한다.\n","\n","예를들어 가중치 매개변수의 손실 함수의 미분이란?   \n","> 가중치 매개변수의 값을 아주 조금 변화 시켰을 때, 손실 함수가 어떻게 변하는지 볼 수 있다.\n","\n","만약 미분 값이 음수면 가중치 매개변수를 양의 방향으로 변화 시켜 손실 함수의 값을 줄일 수 있다.   \n","반대로 미분 값이 양수면 가중치 매개변수를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다.   \n","그러나 미분 값이 0이면 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수의 값은 줄어들지 않는다.   \n","그래서 가중치 매개변수의 갱신은 거기서 멈춘다.\n","\n","정확도를 지표로 삼으면 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없게 된다.   \n","이를 구체적인 예를 통해 알아보자.   \n","한 신경망이 100장의 훈련 데이터 중 32장을 올바로 인식한다면 정확도는 32%이다.   \n","정확도가 지표였다면 가중치 매개변수의 값을 조금 조절해도 정확도는 32%이다.   \n","즉, 매개변수를 약간만 조정해서는 정확도가 개선되지 않는다.   \n","만약 개선된다 하더라도 그 값은 32.0123%와 같은 연속적인 변화보다는 33%, 34%처럼 불연속적인 값으로 바뀌어버린다.\n","\n","손실 함수를 지표로 삼으면 어떻게 바뀔까?   \n","현재 손실 함수의 값은 0.92543... 같은 수치이다.   \n","구리고 매개변수의 값이 조금 변하면 그에 반응하여 손실 함수의 값도 0.93432... 처럼 연속적으로 변화한다.\n","\n","정리하자면 정확도는 매개변수의 사소한 변화에 거의 반응을 보이지 않고, 반응이 있더라도 그 값이 불 연속적이다.   \n","이는 계단 함수를 활성화 함수로 사용하지 않는 이유와 동일하다.   \n","만약 활성화 함수로 계단 함수를 사용하면 신경망 학습이 잘 이뤄지지 않는다.   \n","계단 함수의 미분은 0이 아닌 장소에서 0이다.   \n","\n","계단 함수는 한순간만 변화를 일으키지만, 시그모이드 함수의 미분은 연속적으로 변하고 곡선의 기울기도 연속적으로 변한다.   \n","즉 시그모이드 함수의 미분은 어느 장소라도 0이 되지는 않는다.   \n","신경망 학습에서 중요한 성질로, 기울기가 0이 되지 않아 신경망이 올바르게 학습할 수 있다."],"metadata":{"id":"5jAsxgpVwuru"}},{"cell_type":"markdown","source":["# 4.3 수치 미분"],"metadata":{"id":"hOE0ujqx6FsR"}},{"cell_type":"markdown","source":["### 4.3.1 미분"],"metadata":{"id":"CMREgGrT6GlE"}},{"cell_type":"markdown","source":["### 4.3.2 수치 미분의 예"],"metadata":{"id":"YJRS36ai6Jx3"}},{"cell_type":"markdown","source":["### 4.3.3 편미분"],"metadata":{"id":"xEmaYq6i6LJA"}},{"cell_type":"markdown","source":["# 4.4 기울기"],"metadata":{"id":"QrYxo6SQ6NEB"}},{"cell_type":"markdown","source":["### 4.4.1 경사법(경사 하강법)"],"metadata":{"id":"R1QiX0ke6N8K"}},{"cell_type":"markdown","source":["### 4.4.2 신경망에서의 기울기"],"metadata":{"id":"kFSz14Q-6R5m"}},{"cell_type":"markdown","source":["# 4.5 학습 알고리즘 구현하기"],"metadata":{"id":"hCiEr7QJ6THr"}},{"cell_type":"markdown","source":["### 4.5.1 2층 신경망 클래스 구현하기"],"metadata":{"id":"qc9iyD6E6V3_"}},{"cell_type":"markdown","source":["### 4.5.2 미니배치 학습 구현하기"],"metadata":{"id":"L7Yet7nR6Yr_"}},{"cell_type":"markdown","source":["### 4.5.3 시험 데이터로 평가하기"],"metadata":{"id":"Z-qPaQ2L6akw"}}]}