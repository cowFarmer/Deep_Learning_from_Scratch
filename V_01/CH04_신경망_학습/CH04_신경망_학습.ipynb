{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CH04_신경망_학습.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1cYwKIMQcfF-cNcF73ABnroPoU6udSHtY","authorship_tag":"ABX9TyM6cGoWsJZVhJ6n8EAfcQrO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# 4. 신경망 학습\n","학습을 통해 가중치 매개변수의 최적값을 자동으로 획득해보자.   \n","신경망을 학습할 수 있게 해주는 지표인 손실 함수를 통해, 손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 학습의 목표이다.   \n","함수의 기울기를 활용하는 경사법을 알아보자."],"metadata":{"id":"unGyDlE-Hv5y"}},{"cell_type":"markdown","source":["# 4.1 데이터에서 학습한다!\n","신경망은 데이터를 보고 가중치 매개변수의 값을 자동으로 결정한다.   \n","신경망 학습과 MNIST 데이터셋의 손글씨 숫자를 학습하는걸 코드로 구현해보자."],"metadata":{"id":"k_f6hdXZHycq"}},{"cell_type":"markdown","source":["### 4.1.1 데이터 주도 학습\n","머신러닝은 데이터가 생명이다.   \n","데이터에서 패턴을 발견해 답을 찾는다.\n","\n","보통 문제를 해결하기 위해, 사람이 생각하고 답을 찾는게 일반적이다.   \n","그리고 사람의 경험과 직관을 단서로 시행착오를 거듭하며 일을 진행한다.   \n","하지만 기계학습은 사람의 개입을 최소화하고 데이터로부터 패턴을 찾으려 시도한다.   \n","특히 신경망과 딥러닝은 기존 기계학습보다 사람의 개입을 더욱 배제한다.\n","\n","사람이 숫자를 분류하는 프로그램을 설계하려면, 그 안에 숨은 규칙성을 명확한 로직으로 풀기 어렵다.   \n","그래서 신경망은 이미지에서 __특징(feature)__을 추출하고 특징의 패턴을 기계학습 기술로 학습한다.   \n","여기서 특징은 입력 데이터에서 중요한 데이터를 정확하게 추출할 수 있도록 설계된 변환기를 가리킨다.\n","\n","기계학습은 모아진 데이터로부터 기계가 규칙을 찾아낸다.   \n","하지만 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 사람이 설계해야 한다.   \n","이는 문제에 적합한 특징을 사용하지 않으면 좋은 결과를 얻을 수 없음을 뜻한다.   \n","즉 기계학습도 사람이 적절한 특징을 생각해야한다.   \n","\n","신경망은 이미지에 포함된 중요한 특징까지 기계가 스스로 학습하게 된다.   \n","신경망의 이점으로 모든 문제를 같은 맥락에서 풀 수 있고, 주어진 문제의 패턴을 발견하려 시도한다.   \n","즉, 모든 문제를 주어딘 데이터 그대로를 입력으로 활용해 end-to-end로 학습할 수 있다."],"metadata":{"id":"D2vJhjBuIgs7"}},{"cell_type":"markdown","source":["### 4.1.2 훈련 데이터와 시험 데이터\n","기계학습은 범용 능력(아직 보지 못한 데이터)을 제대로 획득하는 것이 최종 목표이다.   \n","그래서 데이터셋을 훈련 데이터와 시험 데이터를 나눠서 평가한다.   \n","하지만 데이터셋 하나로만 학습과 평가를 진행하면 수중의 데이터셋만 지나치게 최적화된 오버피팅 현상이 나올 수도 있다."],"metadata":{"id":"d3AsFtZHQV6G"}},{"cell_type":"markdown","source":["# 4.2 손실 함수\n","신경망에서 현재의 상태를 하나의 지표로 표현한다.   \n","그리고 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색해야한다.   \n","신경망 학습에서 사용하는 지표는 __손실 함수(loss function)__이다.   \n","일반적으로 오차제곱합, 교체 엔트로피 오차를 사용한다."],"metadata":{"id":"-7G5Mn3oTBQp"}},{"cell_type":"markdown","source":["### 4.2.1 오차제곱합\n","__오차제곱합(sum of squares for error, SSE)__을 알아보자.   \n","$E = {{1}\\over{2}}\\sum\\limits_k(y_k-t_k)^2$   \n","$y_k$는 신경망의 출력, $t_k$는 정답 레이블, $k$는 데이터의 차원 수를 나타낸다."],"metadata":{"id":"oLJ0L674VPTW"}},{"cell_type":"markdown","source":["여기서 y는 소프트맥스의 출력으로 확률로 해석하자.   \n","그리고 t는 정답만 1로 나타내는 원-핫 인코딩 형태이다."],"metadata":{"id":"1u0tV6NNW0kJ"}},{"cell_type":"code","source":["import numpy as np\n","\n","def sum_squares_error(y, t):\n","    return 0.5 * np.sum((y-t)**2)"],"metadata":{"id":"2yXpPTBLXDQv","executionInfo":{"status":"ok","timestamp":1657161632411,"user_tz":-540,"elapsed":61,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"TmzHqBJxBpcu","executionInfo":{"status":"ok","timestamp":1657161632412,"user_tz":-540,"elapsed":60,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"outputs":[],"source":["y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"]},{"cell_type":"code","source":["sum_squares_error(np.array(y), np.array(t))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmednmPDXM3Q","executionInfo":{"status":"ok","timestamp":1657161632413,"user_tz":-540,"elapsed":61,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"c04469e4-c04a-4e11-f65a-5b9affe98d16"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.09750000000000003"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","\n","sum_squares_error(np.array(y), np.array(t))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qNwr5CMeXskj","executionInfo":{"status":"ok","timestamp":1657161632413,"user_tz":-540,"elapsed":49,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"9da816b1-a563-42d5-b6ff-966874bfc306"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5975"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["신경망이 추정한 값과 정답이 일치한 상황과 그렇지 않은 상황에 대해 SSE 값을 출력해봤다."],"metadata":{"id":"KQodP_2kX35J"}},{"cell_type":"markdown","source":["### 4.2.2 교차 엔트로피 오차\n","손실 함수로 __교차 엔트로피 오차(cross entropy error, CEE)__를 많이 사용하므로 확인해보자.\n","\n","$E = -\\sum\\limits_kt_klogy_k$\n","\n","여기서 log는 밑이 $e$인 자연로그($log_e$)이다.   \n","$y_k$는 신경망의 출력(추정값), $t_k$는 정답 레이블이고 원-핫 인코딩이다.   \n","그래서 실질적으로 정답일 때 추정($t_k$가 1일때 $y_k$)의 자연로그를 계산하게 된다.   \n","예를들면 정답 레이블에 맞는 신경망의 출력 $y_k$가 0.6이면 교차 엔트로피 오차는 -log0.6 = 0.51이 된다.   \n","같은 조건에서 신경망 출력이 0.1이면 -log0.1 = 2.30이 된다.   \n","\n","![](https://support.minitab.com/ko-kr/minitab/18/naturallog_scatterplot.png)   \n","자연로그의 그래프를 확인해보면 x가 1일때 y가 0이되고, x가 0에 가까워질수록 y의 값은 점점 작아진다."],"metadata":{"id":"brx5ply5YGsM"}},{"cell_type":"code","source":["def cross_entropy_error(y, t):\n","    delta = 1e-7\n","    return -np.sum(t * np.log(y + delta))"],"metadata":{"id":"cfY2h4YqaUSJ","executionInfo":{"status":"ok","timestamp":1657161632413,"user_tz":-540,"elapsed":42,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["np.log() 함수에 0을 넣으면 마이너스 무한대인 -inf가 나와 계산을 진행할 수 없기에, 아주 작은 값인 delta를 넣어줬다."],"metadata":{"id":"nS2rV6Q6adRY"}},{"cell_type":"code","source":["y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","\n","cross_entropy_error(np.array(y), np.array(t))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UiyQ7eGjac4v","executionInfo":{"status":"ok","timestamp":1657161632413,"user_tz":-540,"elapsed":42,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"18a5786b-605e-4ea3-827c-61ee65145669"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.510825457099338"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","\n","cross_entropy_error(np.array(y), np.array(t))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oIA4tHmPgje9","executionInfo":{"status":"ok","timestamp":1657161632414,"user_tz":-540,"elapsed":41,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"4aa4ad26-0de4-455e-8275-65993d4cf1a5"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.302584092994546"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["### 4.2.3 미니배치 학습\n","기계학습 문제는 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다.   \n","즉, 훈련 데이터 개수만큼 이들의 손실 함수 값들의 합을 지표로 삼는다.   \n","\n","\b훈련 데이터 모두에 대한 손실 함수의 합을 구하는 방법을 알아보자.   \n","$E = -{{1}\\over{N}}\\sum\\limits_n\\sum\\limits_kt_{nk}logy_{nk}$ \n","\n","데이터가 $N$개이고 $t_{nk}$는 $n$번째 데이터의 $k$번째 값을 의미한다.   \n","그리고 $N$으로 나누어 정규화를 하고, $N$으로 나눔으로써 __평균 손실 함수__를 구하는 것이다.\n","\n","MNIST의 데이터셋은 훈련 데이터가 60000개였다.   \n","모든 데이터를 대상으로 손실 함수의 합을 구하려면 시간이 걸린다.   \n","그래서 데이터의 일부를 추려 전체의 근사치로 이용할 수 있다.   \n","이 일부를 __미니 배치__라고 한다.   \n","즉, 6만장의 훈련 데이터 중에서 100장을 무작위로 뽑아 학습하는 것이다."],"metadata":{"id":"vQgPW8tcgz1V"}},{"cell_type":"markdown","source":["### MNIST dataset 불러오기"],"metadata":{"id":"xnselBSBvBlx"}},{"cell_type":"code","source":["# coding: utf-8\n","try:\n","    import urllib.request\n","except ImportError:\n","    raise ImportError('You should use Python 3.x')\n","import os.path\n","import gzip\n","import pickle\n","import os\n","import numpy as np\n","\n","\n","url_base = 'http://yann.lecun.com/exdb/mnist/'\n","key_file = {\n","    'train_img':'train-images-idx3-ubyte.gz',\n","    'train_label':'train-labels-idx1-ubyte.gz',\n","    'test_img':'t10k-images-idx3-ubyte.gz',\n","    'test_label':'t10k-labels-idx1-ubyte.gz'\n","}\n","\n","dataset_dir = os.path.dirname('/content/drive/Othercomputers/MacBook_Air/Deep_Learning_from_Scratch/V_01/dataset')\n","save_file = dataset_dir + \"/mnist.pkl\"\n","\n","train_num = 60000\n","test_num = 10000\n","img_dim = (1, 28, 28)\n","img_size = 784\n","\n","\n","def _download(file_name):\n","    file_path = dataset_dir + \"/\" + file_name\n","\n","    if os.path.exists(file_path):\n","        return\n","\n","    print(\"Downloading \" + file_name + \" ... \")\n","    headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\"}\n","    request = urllib.request.Request(url_base+file_name, headers=headers)\n","    response = urllib.request.urlopen(request).read()\n","    with open(file_path, mode='wb') as f:\n","        f.write(response)\n","    print(\"Done\")\n","\n","def download_mnist():\n","    for v in key_file.values():\n","       _download(v)\n","\n","def _load_label(file_name):\n","    file_path = dataset_dir + \"/\" + file_name\n","\n","    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n","    with gzip.open(file_path, 'rb') as f:\n","            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n","    print(\"Done\")\n","\n","    return labels\n","\n","def _load_img(file_name):\n","    file_path = dataset_dir + \"/\" + file_name\n","\n","    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n","    with gzip.open(file_path, 'rb') as f:\n","            data = np.frombuffer(f.read(), np.uint8, offset=16)\n","    data = data.reshape(-1, img_size)\n","    print(\"Done\")\n","\n","    return data\n","\n","def _convert_numpy():\n","    dataset = {}\n","    dataset['train_img'] =  _load_img(key_file['train_img'])\n","    dataset['train_label'] = _load_label(key_file['train_label'])\n","    dataset['test_img'] = _load_img(key_file['test_img'])\n","    dataset['test_label'] = _load_label(key_file['test_label'])\n","\n","    return dataset\n","\n","def init_mnist():\n","    download_mnist()\n","    dataset = _convert_numpy()\n","    print(\"Creating pickle file ...\")\n","    with open(save_file, 'wb') as f:\n","        pickle.dump(dataset, f, -1)\n","    print(\"Done!\")\n","\n","def _change_one_hot_label(X):\n","    T = np.zeros((X.size, 10))\n","    for idx, row in enumerate(T):\n","        row[X[idx]] = 1\n","\n","    return T\n","\n","\n","def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n","    if not os.path.exists(save_file):\n","        init_mnist()\n","\n","    with open(save_file, 'rb') as f:\n","        dataset = pickle.load(f)\n","\n","    if normalize:\n","        for key in ('train_img', 'test_img'):\n","            dataset[key] = dataset[key].astype(np.float32)\n","            dataset[key] /= 255.0\n","\n","    if one_hot_label:\n","        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n","        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n","\n","    if not flatten:\n","         for key in ('train_img', 'test_img'):\n","            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n","\n","    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label'])\n","\n","\n","if __name__ == '__main__':\n","    init_mnist()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E0s3khiSu_U3","executionInfo":{"status":"ok","timestamp":1657161641904,"user_tz":-540,"elapsed":9529,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"69874de1-0c96-404f-d2d4-78122dd42c20"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Converting train-images-idx3-ubyte.gz to NumPy Array ...\n","Done\n","Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n","Done\n","Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n","Done\n","Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n","Done\n","Creating pickle file ...\n","Done!\n"]}]},{"cell_type":"code","source":["import sys, os\n","# sys.path.append(os.pardir)\n","import numpy as np\n","# from dataset.mnist import load_mnist\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","print(x_train.shape)\n","print(t_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7c0kH0iTgrF_","executionInfo":{"status":"ok","timestamp":1657161642626,"user_tz":-540,"elapsed":967,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"b77766df-5bb9-41ca-ae04-9c253eaf73ed"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 784)\n","(60000, 10)\n"]}]},{"cell_type":"code","source":["train_size = x_train.shape[0]\n","batch_size = 10\n","batch_mask = np.random.choice(train_size, batch_size)\n","x_batch = x_train[batch_mask]\n","t_batch = t_train[batch_mask]"],"metadata":{"id":"t7Ek6LTyuGkD","executionInfo":{"status":"ok","timestamp":1657161642626,"user_tz":-540,"elapsed":90,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["np.random.choice(60000, 10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1RN4uG-u6gr","executionInfo":{"status":"ok","timestamp":1657161642627,"user_tz":-540,"elapsed":90,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"2c8cebbb-0171-43b9-c0f1-84b6d9454c98"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([38080, 45946, 12639,  7418,  9587, 56739, 57939, 26367, 46533,\n","       45232])"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["훈련 데이터에서 무작위로 batch_size만큼 꺼내올 수 있다.   \n","np.random.choice()로 랜덤으로 선택해 인덱스로 사용하여 미니배치를 뽑아내면 된다."],"metadata":{"id":"npTw_C2Du7I2"}},{"cell_type":"markdown","source":["### 4.2.4 (배치용) 교차 엔트로피 오차 구현하기\n","교차 엔트로피 오차에서 배치 데이터를 구현할 수 있게 해보자.   \n","예측한 y의 값이 1차원인 데이터 하나당 교차 엔트로피 오차를 구하는 경우에는 reshape로 데이터의 형상을 바꿔준다.   \n","그리고 배치 크기로 나눠 정규화하고 이미지 1장당 평균의 교차 엔트로피 오차를 계산한다."],"metadata":{"id":"_CA1XFwlwr-r"}},{"cell_type":"code","source":["def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","\n","    batch_size = y.shape[0]\n","    return -np.sum(t * np.log(y + 1e-7)) / batch_size"],"metadata":{"id":"x4zkM_QMxQTV","executionInfo":{"status":"ok","timestamp":1657161642630,"user_tz":-540,"elapsed":91,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["정답이 원-핫 인코딩이 아닌 숫자 레이블로 주어졌을 때 교차 엔트로피 오차를 어떻게 구현할까?"],"metadata":{"id":"3FyTDvNsx5T4"}},{"cell_type":"code","source":["def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","    \n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"],"metadata":{"id":"UMq62dmhyut1","executionInfo":{"status":"ok","timestamp":1657161642631,"user_tz":-540,"elapsed":92,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["이 구현에서 원-핫 인코딩일 때 t가 0인 원소는 교차 엔트로피 오차도 0이므로, 그 계산은 무시해도 좋다는 것이 핵심이다.   \n","이는 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산할 수 있다.   \n","그래서 원-핫 인코딩시 `t*np.log(y)`를 레이블 표현은 `np.log(y[np.arange(batch_size), t])`로 구현한다.\n","\n","`np.log(y[np.arange(batch_size), t])`에서 np.arange(batch_size)는 0부터 batch_size-1까지 배열을 생성한다.   \n","t에는 레이블이 [2,7,0,9,4]와 같이 저장되어 있으므로 `y[np.arange(batch_size), t]`는 각 데이터의 정답 레이블에 해당하는 신경망의 출력을 추출한다.   \n","(예를들자면 `[y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]`인 넘파이 배열을 생성한다.)"],"metadata":{"id":"VFJJRFCnzh_y"}},{"cell_type":"markdown","source":["### 4.2.4 왜 손실 함수를 설정하는가?\n","궁극적인 목표는 높은 정확도를 끌어내는 매개변수 값을 찾는 것이다.   \n","그렇다면 왜 정확도 지표를 두고 손실 함수의 값을 택할까?\n","\n","신경망 학습에서 __미분__의 역할이 있기 때문이다.   \n","신경망 학습에서 최적의 매개변수(가중치, 편향)을 탐색할 때 손실 함수의 값을 가능한 작게 하는 매개변수 값을 찾는다.   \n","이때 매개변수의 기울기를 미분을 통해 계산하고, 그 미분 값을 단서로 매개변수의 값을 갱신한다.\n","\n","예를들어 가중치 매개변수의 손실 함수의 미분이란?   \n","> 가중치 매개변수의 값을 아주 조금 변화 시켰을 때, 손실 함수가 어떻게 변하는지 볼 수 있다.\n","\n","만약 미분 값이 음수면 가중치 매개변수를 양의 방향으로 변화 시켜 손실 함수의 값을 줄일 수 있다.   \n","반대로 미분 값이 양수면 가중치 매개변수를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다.   \n","그러나 미분 값이 0이면 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수의 값은 줄어들지 않는다.   \n","그래서 가중치 매개변수의 갱신은 거기서 멈춘다.\n","\n","정확도를 지표로 삼으면 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없게 된다.   \n","이를 구체적인 예를 통해 알아보자.   \n","한 신경망이 100장의 훈련 데이터 중 32장을 올바로 인식한다면 정확도는 32%이다.   \n","정확도가 지표였다면 가중치 매개변수의 값을 조금 조절해도 정확도는 32%이다.   \n","즉, 매개변수를 약간만 조정해서는 정확도가 개선되지 않는다.   \n","만약 개선된다 하더라도 그 값은 32.0123%와 같은 연속적인 변화보다는 33%, 34%처럼 불연속적인 값으로 바뀌어버린다.\n","\n","손실 함수를 지표로 삼으면 어떻게 바뀔까?   \n","현재 손실 함수의 값은 0.92543... 같은 수치이다.   \n","구리고 매개변수의 값이 조금 변하면 그에 반응하여 손실 함수의 값도 0.93432... 처럼 연속적으로 변화한다.\n","\n","정리하자면 정확도는 매개변수의 사소한 변화에 거의 반응을 보이지 않고, 반응이 있더라도 그 값이 불 연속적이다.   \n","이는 계단 함수를 활성화 함수로 사용하지 않는 이유와 동일하다.   \n","만약 활성화 함수로 계단 함수를 사용하면 신경망 학습이 잘 이뤄지지 않는다.   \n","계단 함수의 미분은 0이 아닌 장소에서 0이다.   \n","\n","계단 함수는 한순간만 변화를 일으키지만, 시그모이드 함수의 미분은 연속적으로 변하고 곡선의 기울기도 연속적으로 변한다.   \n","즉 시그모이드 함수의 미분은 어느 장소라도 0이 되지는 않는다.   \n","신경망 학습에서 중요한 성질로, 기울기가 0이 되지 않아 신경망이 올바르게 학습할 수 있다."],"metadata":{"id":"5jAsxgpVwuru"}},{"cell_type":"markdown","source":["# 4.3 수치 미분\n","경사법은 기울기 값을 기준으로 나아갈 방향을 정한다.   \n","기울기를 알기 위해 미분부터 복습해보자."],"metadata":{"id":"hOE0ujqx6FsR"}},{"cell_type":"markdown","source":["### 4.3.1 미분\n","10분동안 2km씩 달렸다고 가정해보자.   \n","속도는 2 / 10 = 0.2km/분 으로 계산할 수 있다.   \n","\n","위 예에서는 달린 거리가 시간에 대해 얼마나 변화했는지 계산한다.   \n","10분에 2km를 뛰었다는 것은, 10분 동안의 평균 속도를 구한 것이다.   \n","__미분은 특정 순간의 변화량을 뜻한다.__   \n","그래서 10분이라는 시간을 가능한 줄여 한 순간의 변화량을 얻는 것이다.   \n","\n","미분의 수식은 다음과 같다.   \n","${df(x)\\over{dx}} = \\underset{h->0}{lim}{f(x+h)-f(x)\\over{h}}$\n","\n","좌변 $f(x)$의 $x$에 대한 미분($x$에 대한 $f(x)$의 변화량)을 나타내는 기호이다.   \n","이는 $x$의 작은 변화가 함수 $f(x)$를 얼마나 변화시키느냐를 의미한다.   \n","이때 시간을 뜻하는 $h$를 한없이 0에 가깝게 한다는 의미로 $\\underset{h->0}{lim}$를 나타낸다."],"metadata":{"id":"CMREgGrT6GlE"}},{"cell_type":"code","source":["# 미분 나쁜 구현의 예\n","def numerical_diff(f, x):\n","    h = 1e-50\n","    return (f(x + h) - f(x)) / h"],"metadata":{"id":"6-AqjPZ5oiVn","executionInfo":{"status":"ok","timestamp":1657161642631,"user_tz":-540,"elapsed":92,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["함수의 이름은 수치 미분(numerical differentiation)이다.   \n","두 가지의 인수(함수 f와 f에 넘길 인수 x)를 받는다.   \n","문제가 없어 보일수도 있지만 개선해야 할 점이 2개가 있다.\n","\n","첫번째로 $h$에 작은 값을 대입하기 위해 1e-50을 사용했다.   \n","하지만 이는 반올림 오차 문제를 일으킨다.   \n","이는 소수점 8자리 이하가 생략되어 최종 계산 결과에 오차가 생기게 한다."],"metadata":{"id":"_voa_ucUoyfd"}},{"cell_type":"code","source":["np.float32(1e-50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"teaDoYHUp-aA","executionInfo":{"status":"ok","timestamp":1657161642631,"user_tz":-540,"elapsed":91,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"8ce7834f-d936-4f24-90cc-9488252d46ae"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["너무 작은 값을 이용하면 컴퓨터로 계산하는데 문제가 된다.   \n","그래서 컴퓨터가 이해할 수 있는 `h = 10-4`로 바꾸자.   \n","\n","두번째는 함수 f와 차분과 관련된 것이다.   \n","x + h와 x 사이의 함수 f 차분을 계산하고 있지만 이는 오류가 있다.   \n","\n","![](https://mblogthumb-phinf.pstatic.net/MjAxODA2MTVfMTQ1/MDAxNTI5MDUyMjE0MTkw.Gye5r5O_sHAWk79HZbGyQpi7rE4Gwmb3bCVRXTrvj6Ig.FmqyWXoIfUXckKwoPPPOQza0ZRz5G7jtcMGol1ypjlUg.PNG.ssdyka/fig_4-5.png?type=w2)   \n","진정한 미분은 위 그림과 같이 x 위치의 함수의 기울기에 해당하지만, 구현한 미분은 x + h와 x사이의 기울기에 해당된다.   \n","그래서 진정한 미분과 위에 구현한 값은 엄밀히 일치하지 않는다.   \n","이 차이는 $h$를 무한히 0으로 좁히는 것이 불가능해 생기는 한계이다.\n","\n","그래서 이러한 오차를 줄이기 위해 $(x + h)$와 $(x - h)$일 때 함수 $f$의 차분을 계산하는 방법을 쓰기도 한다.   \n","이 차분은 $x$를 중심으로 전후의 차분을 계산한다는 의미로 __중심 차분__ 혹은 __중앙 차분__이라 한다."],"metadata":{"id":"GWoU1KLCqBCN"}},{"cell_type":"code","source":["def numerical_diff(f, x):\n","    h = 1e-4 # 0.0001\n","    return (f(x+h) - f(x-h)) / (2*h)"],"metadata":{"id":"locjHlPP7USI","executionInfo":{"status":"ok","timestamp":1657161642631,"user_tz":-540,"elapsed":89,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["> 아주 작은 차분으로 미분하는 것을 수치 미분이라고 하며 근사치로 계산한다.   \n","수식을 전개해 미분하는 것은 해석적 미분이라고 표현한다.   \n","해석적 미분은 오차를 포함하지 않는 진정한 미분 값을 구해준다."],"metadata":{"id":"yVT6ZtCj9TNs"}},{"cell_type":"markdown","source":["### 4.3.2 수치 미분의 예\n","$y = 0.01x^2 + 0.1x$   \n","다음과 같은 2차 함수를 수치 미분해보자."],"metadata":{"id":"YJRS36ai6Jx3"}},{"cell_type":"code","source":["def function_1(x):\n","    return 0.01*x**2 + 0.1*x"],"metadata":{"id":"Qws60Rfj-F-a","executionInfo":{"status":"ok","timestamp":1657161642631,"user_tz":-540,"elapsed":89,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pylab as plt\n","\n","x = np.arange(0.0, 20.0, 0.1)\n","y = function_1(x)\n","plt.xlabel('x')\n","plt.ylabel('f(x)')\n","plt.plot(x, y)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"QLVZbpoi-N7o","executionInfo":{"status":"ok","timestamp":1657161642632,"user_tz":-540,"elapsed":88,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"08e02fe4-d52a-45d2-f7af-a1cc766452eb"},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["print(numerical_diff(function_1, 5))\n","print(numerical_diff(function_1, 10))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G9P5FWrPix8r","executionInfo":{"status":"ok","timestamp":1657161642632,"user_tz":-540,"elapsed":82,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"086506c5-f730-4270-e626-d77f41622dfe"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["0.1999999999990898\n","0.2999999999986347\n"]}]},{"cell_type":"markdown","source":["미분 값이 $x$에 대한 $f(x)$의 변화량이다.   \n","즉, 함수의 기울기에 해당한다.   \n","$f(x) = 0.01x^2 + 0.1x$의 해석적 해는 ${df(x)\\over{dx}} = 0.02x + 0.1$이다.   \n","그래서 $x$가 5와 10일때의 진정한 미분은 0.2, 0.3이 된다.   \n","이 오차는 매우 작고, 실제로 거의 같은 값이라고 해도 될 만큼 작은 오차이다.\n","\n","![](https://blog.kakaocdn.net/dn/rtr23/btqL6I0GC4t/ADSyT1zvsGHxUtB8HiVIkK/img.png)"],"metadata":{"id":"9y6AvMnUmKzX"}},{"cell_type":"markdown","source":["### 4.3.3 편미분\n","다음과 같은 수식을 구현해서 편미분을 알아보자.   \n","$f(x_0, x_1) = x^2_0 + x^2_1$"],"metadata":{"id":"xEmaYq6i6LJA"}},{"cell_type":"code","source":["def function_2(x):\n","    return x[0]**2 + x[1]**2\n","    # return np.sum(x**2)"],"metadata":{"id":"veZD1XBGntiZ","executionInfo":{"status":"ok","timestamp":1657161642632,"user_tz":-540,"elapsed":78,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["위 식은 다음과 같은 3차원으로 그려진다.   \n","![](https://velog.velcdn.com/images%2Fksj5738%2Fpost%2F0269fe78-698c-4c01-8f7d-4cf252fcee0f%2Fimage.png)\n","\n","수치 미분의 예에서 주의할 점은 변수가 2개라는 것이다.   \n","그래서 $x_0$, $x_1$중 어느 변수에 대한 미분인지 구별해야 한다.   \n","그리고 변수가 여럿인 함수에 대한 미분을 편미분이라고 한다.   \n","수식으로 ${\\alpha f\\over{\\alpha x_0}}$나 ${\\alpha f\\over{\\alpha x_1}}$처럼 쓴다.\n","\n","다음 문제를 풀어보자.   \n","$x_0$ = 3, $x_1$ = 4일때 $x_0$에 대한 편미분 ${\\alpha f \\over{\\alpha x_0}}$을 구하라."],"metadata":{"id":"UaXjuTEUoKEi"}},{"cell_type":"code","source":["def function_tmp1(x0):\n","    return x0*x0 + 4.0**2.0\n","\n","numerical_diff(function_tmp1, 3.0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9uoy-u4JuHk","executionInfo":{"status":"ok","timestamp":1657161642632,"user_tz":-540,"elapsed":77,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"308e4599-d0fe-4cd0-b1c8-0aabd9c3be3f"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6.00000000000378"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["$x_0$ = 3, $x_1$ = 4일때 $x_1$에 대한 편미분 ${\\alpha f \\over{\\alpha x_1}}$을 구하라."],"metadata":{"id":"zDZH8bKZKbKp"}},{"cell_type":"code","source":["def function_tmp2(x1):\n","    return 3.0**2.0 + x1*x1\n","\n","numerical_diff(function_tmp2, 4.0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MyanVZmGLYW1","executionInfo":{"status":"ok","timestamp":1657161642632,"user_tz":-540,"elapsed":74,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"0c47a4c0-933b-4e5a-a312-48747a2a471a"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7.999999999999119"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["위 문제의 결과처럼 편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구한다.   \n","여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정한다."],"metadata":{"id":"p7VLg03jMBYP"}},{"cell_type":"markdown","source":["# 4.4 기울기\n","지금까지 $x_0$, $x_1$의 편미분을 변수별로 따로 계산했는데 이들을 동시에 계산하고 싶으면 어떻게 할까?   \n","$x_0 = 3, x_1 = 4$일때 $(x_0, x_1)$ 양쪽의 편미분을 묶어서 ${({\\alpha f\\over{\\alpha x_0}}, {\\alpha f\\over{\\alpha x_1}})}$처럼 모든 변수의 편미분을 벡터로 정리한 것을 __기울기(gradient)__라고 한다."],"metadata":{"id":"QrYxo6SQ6NEB"}},{"cell_type":"code","source":["def numerical_gradient(f, x):\n","    h = 1e-4\n","    grad = np.zeros_like(x) # x와 형상이 같은 배열 생성\n","\n","    for idx in range(x.size):\n","        tmp_val = x[idx]\n","        # f(x+h) 계산\n","        x[idx] = tmp_val + h\n","        fxh1 = f(x)\n","\n","        # f(x-h) 계산\n","        x[idx] = tmp_val - h\n","        fxh2 = f(x)\n","\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\n","        x[idx] = tmp_val # 값 복원\n","\n","    return grad"],"metadata":{"id":"_9GGAysKM59y","executionInfo":{"status":"ok","timestamp":1657161642632,"user_tz":-540,"elapsed":70,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n","print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n","print(numerical_gradient(function_2, np.array([3.0, 0.0])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rg_3dGV8S6mt","executionInfo":{"status":"ok","timestamp":1657161642633,"user_tz":-540,"elapsed":71,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"da9af795-d2d2-4188-cb78-a532f2e55d20"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[6. 8.]\n","[0. 4.]\n","[6. 0.]\n"]}]},{"cell_type":"markdown","source":["세 점 (3,4), (0,2), (3,0)에서의 기울기를 구해봤다.   \n","그렇다면 기울기의 의미는 무엇일까?\n","\n","![](https://blog.kakaocdn.net/dn/TCkEA/btqIqX12ivj/kgkKLdIBGqt4VNkumMdSck/img.png)   \n","$f(x_0,x_1) = x^2_0+x^2_1$의 기울기   \n","다음과 같이 방향을 가진 벡터(화살표)로 기울기 그림이 그려진다.   \n","기울기는 함수의 가장 낮은 장소(최솟값)을 가리키는 것 같다.\n","\n","__정확히는 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다.__"],"metadata":{"id":"aljuQvFpU-F_"}},{"cell_type":"markdown","source":["### 4.4.1 경사법(경사 하강법)\n","기계학습 문제 대부분은 학습 단계에서 최적의 매개변수를 찾아낸다.   \n","신경망도 최적의 매개변수 가중치와 편향을 학습하면서 찾아야 한다.   \n","여기서 최적이란 손실 함수가 최솟값이 될 때의 매개변수 값이다.   \n","일반적으로 매개변수 공간이 광대하여 어디가 최솟값이 되는 곳인지 짐작할 수 없다.   \n","이런 상황에서 기울기를 이용해 함수의 최솟값을 찾으려는 것이 경사법이다.\n","\n","주의할 점은 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 기울기라는 것이다.   \n","그러나 기울기가 가리키는 곳이 함수의 최솟값이 있는지 보장할 수 없다.   \n","실제로 복잡한 함수는 기울기가 가리키는 방향에 최솟값이 없는 경우가 대부분이다.\n","\n","> 함수가 극솟값(local minimum), 최솟값(global minimum), 안장점(saddle point)가 되는 장소에서 기울기가 0이다.   \n","그래서 경사법은 기울기가 0인 장소를 찾미나 그것이 반드시 최솟값이라고는 할 수 없다. \n","\n","기울어진 방향이 최솟값을 가리키는 것은 아니나, 그 방향으로 가야 함수 값을 줄일 수 있다.   \n","그래서 경사법을 통해 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다.   \n","그리고 이동한 장소에서 다시 기울기를 구하고, 다시 나아가기를 반복한다.   \n","이렇게 함수의 값을 점차 줄이는 것이 __경사법__이다.\n","\n","경사법을 수식을 통해 알아보자.   \n","$x_0 = x_0 - \\eta{\\alpha f\\over{\\alpha x_0}}$   \n","$x_1 = x_1 - \\eta{\\alpha f\\over{\\alpha x_1}}$\n","\n","$\\eta$(에타)는 갱신하는 양을 나타낸다.   \n","그리고 이를 __학습률(learning rate)__이라고 말한다.   \n","학습률은 보통 0.01, 0.001등 미리 특정 값으로 정해야 하는데, 일반적으로 값이 너무 크거나 작으면 좋은 장소를 찾아갈 수 없다.   \n","신경망 학습에서 학습률 값을 변경하면서 올바르게 학습하고 있는지 확인하면서 진행한다."],"metadata":{"id":"R1QiX0ke6N8K"}},{"cell_type":"code","source":["def gradient_descent(f, init_x, lr=0.01, step_num=100):\n","    x = init_x\n","\n","    for i in range(step_num):\n","        grad = numerical_gradient(f, x)\n","        x -= lr * grad\n","    return x"],"metadata":{"id":"hFHYgTkikS0c","executionInfo":{"status":"ok","timestamp":1657161642633,"user_tz":-540,"elapsed":51,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["위와 같이 경사 하강법을 구현할 수 있다.   \n","f는 최적화하려는 함수, init_x는 초깃값을 말한다.   \n","함수의 기울기는 numerical_gradient(f, x)로 구하고 그 기울기에 학습률을 곱한 값으로 갱신하는 처리를 step_num번 반복한다.\n","\n","경사법으로 $f(x_0, x_1) = x^2_0+x^2_1$의 최솟값을 구해보자."],"metadata":{"id":"mE_pNScmkeW4"}},{"cell_type":"code","source":["def function_2(x):\n","    return x[0]**2 + x[1]**2\n","\n","init_x = np.array([-3.0, 4.0])\n","gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vWBxr8QwmUgc","executionInfo":{"status":"ok","timestamp":1657161642633,"user_tz":-540,"elapsed":51,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"3effe47e-58ed-4b1f-9946-3886da734771"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-6.11110793e-10,  8.14814391e-10])"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["거의 (0,0)에 가까운 결과가 나왔다.   \n","진정한 최솟값은 (0,0)이므로 경사법을 통해 거의 정확한 결과를 얻었다.   \n","\n","![](https://compmath.korea.ac.kr/appmath2021/_images/plot_lnn_grad_trace.png)   \n","그림을 통해 경사법을 이용한 갱신 과정을 확인해보자."],"metadata":{"id":"Skz9xkP3m0Sx"}},{"cell_type":"code","source":["# learning rate가 큰 예\n","init_x = np.array([-3.0, 4.0])\n","print(gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100))\n","\n","# learning rate가 작은 예\n","init_x = np.array([-3.0, 4.0])\n","print(gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BOtc2IwDnXwW","executionInfo":{"status":"ok","timestamp":1657161642633,"user_tz":-540,"elapsed":50,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"5b7382dc-2919-4439-93a7-848a2b489970"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["[-2.58983747e+13 -1.29524862e+12]\n","[-2.99999994  3.99999992]\n"]}]},{"cell_type":"markdown","source":["학습률이 너무 크면 큰 값으로 발산하고, 너무 작으면 거의 갱신되지 않은 채 끝난다."],"metadata":{"id":"SMEX0BqgnkLV"}},{"cell_type":"markdown","source":["### 4.4.2 신경망에서의 기울기\n","신경망 학습에서도 기울기를 구해야 한다.   \n","이 기울기는 가중치 매개변수에 대한 손실 함수의 기울기이다.   \n","2x3 shape를 가진 가중치 $W$, 손실 함수 $L$인 신경망을 생각해보자.   \n","이 경우 경사는 ${\\alpha L\\over\\alpha W}$로 표현할 수 있다.\n","\n","$W = ([[w_{11}, w_{12}, w_{13}],[w_{21}, w_{22}, w_{23}]])$   \n","${\\alpha L\\over\\alpha W} = ([[{\\alpha L\\over\\alpha W_{11}}, {\\alpha L\\over\\alpha W_{12}}, {\\alpha L\\over\\alpha W_{13}}], [{\\alpha L\\over\\alpha W_{21}}, {\\alpha L\\over\\alpha W_{22}}, {\\alpha L\\over\\alpha W_{23}}, ]])$\n","\n","1행 1번쨰 원소인 ${\\alpha L\\over\\alpha W_{11}}$은 $w_11$을 조금 변경했을 때 손실 함수 $L$이 얼마나 변화하는지 나타낸다.   \n","중요한 점은 ${\\alpha L\\over\\alpha W}$ 형상이 $W$와 같다.   \n","\n","위 신경망을 예로 실제로 기울기를 구하는 코드를 구현해보자."],"metadata":{"id":"kFSz14Q-6R5m"}},{"cell_type":"code","source":["import numpy as np\n","# import sys, os\n","# sys.path.append(os.pardir)\n","# from common.functions import softmax, cross_entropy_error\n","# from common.gradient import numerical_gradient\n","\n","def softmax(x):\n","    x = x - np.max(x, axis=-1, keepdims=True)\n","    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n","\n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","        \n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","             \n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n","\n","def numerical_gradient(f, x):\n","    h = 1e-4 # 0.0001\n","    grad = np.zeros_like(x)\n","    \n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    while not it.finished:\n","        idx = it.multi_index\n","        tmp_val = x[idx]\n","        x[idx] = tmp_val + h\n","        fxh1 = f(x) # f(x+h)\n","        \n","        x[idx] = tmp_val - h \n","        fxh2 = f(x) # f(x-h)\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\n","        \n","        x[idx] = tmp_val\n","        it.iternext()   \n","        \n","    return grad"],"metadata":{"id":"pje3mYonp6r-","executionInfo":{"status":"ok","timestamp":1657161642633,"user_tz":-540,"elapsed":49,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["class simpleNet:\n","    def __init__(self):\n","        self.W = np.random.rand(2,3) # 정규분포로 초기화\n","\n","    def predict(self, x):\n","        return np.dot(x, self.W)\n","\n","    def loss(self, x, t):\n","        z = self.predict(x)\n","        y = softmax(z)\n","        loss = cross_entropy_error(y, t)\n","\n","        return loss"],"metadata":{"id":"W5-KYq1qqhDH","executionInfo":{"status":"ok","timestamp":1657161642634,"user_tz":-540,"elapsed":49,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["simpleNet 클래스는 형상이 2x3인 가중치 매개변수 하나를 인스턴스 변수로 갖는다.   \n","predict(x)와 loss(x, t)에 들어가는 x는 입력 데이터, t는 정답 레이블이다."],"metadata":{"id":"D8YYc0ZEsGTr"}},{"cell_type":"code","source":["net = simpleNet()\n","print(net.W) # 가중치 매개변수\n","print(net.W.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5IieIQ1ktPpJ","executionInfo":{"status":"ok","timestamp":1657161642634,"user_tz":-540,"elapsed":49,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"a4501e3b-afad-483b-f47f-47a2f9c4a8c0"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.02641031 0.52079936 0.93019882]\n"," [0.06945989 0.80567952 0.24338993]]\n","(2, 3)\n"]}]},{"cell_type":"code","source":["x = np.array([0.6, 0.9])\n","p = net.predict(x)\n","print(p)\n","print(np.argmax(p))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N88AyGAUxm22","executionInfo":{"status":"ok","timestamp":1657161642634,"user_tz":-540,"elapsed":48,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"7644e843-1df8-4f60-921b-322ec57f9052"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.07836009 1.03759118 0.77717022]\n","1\n"]}]},{"cell_type":"code","source":["t = np.array([0,1,0]) # 정답 레이블\n","net.loss(x, t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhaO04lSyFdc","executionInfo":{"status":"ok","timestamp":1657161642634,"user_tz":-540,"elapsed":47,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"6052b824-ee1d-4918-d428-35cf0966841e"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7672866603813425"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["def f(W):\n","    return net.loss(x, t) # y, t\n","\n","dW = numerical_gradient(f, net.W)\n","print(dW)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WTUV-PXD2yJH","executionInfo":{"status":"ok","timestamp":1657161642634,"user_tz":-540,"elapsed":45,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"243332cb-f54d-4d91-aaa9-f102584fe4f9"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.10674165 -0.32143734  0.21469568]\n"," [ 0.16011248 -0.48215601  0.32204353]]\n"]}]},{"cell_type":"markdown","source":["numerical_gradient(f, x)를 사용해 기울기를 구해보자.   \n","여기서 f는 내부에서 f(x)를 실행하는데, 일관성을 위해 f(W)를 정의했다.\n","\n","dW는 2x3의 2차원 배열이다.   \n","여기서 $w_{21}$을 $h$만큼 올리면 손실함수의 값은 0.2$h$만큼 증가한다는 의미를 갖고 있다.   \n","손실 함수를 줄인다는 관점에서 ${\\alpha L \\over \\alpha W_{11}}$은 0.13이니 음의 방향으로 갱신해야하고, ${\\alpha L \\over \\alpha W_{22}}$는 -0.53으로 양의 방향으로 갱신해야 함을 알 수 있다.   \n","그리고 한 번에 갱신되는 양은 ${\\alpha L \\over \\alpha W_{11}}$보다 ${\\alpha L \\over \\alpha W_{22}}$가 크게 기여한다는 사실도 알 수 있다.\n","\n","람다를 쓰면 위 구현을 더 편하게 작성할 수 있다."],"metadata":{"id":"TBfb_J1_2iWj"}},{"cell_type":"code","source":["f = lambda w: net.loss(x, t)\n","dW = numerical_gradient(f, net.W)\n","print(dW)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GsXnbcpC5mjJ","executionInfo":{"status":"ok","timestamp":1657161642634,"user_tz":-540,"elapsed":43,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"a7e39ee5-474e-4e56-d21f-6953a2c7da17"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.10674165 -0.32143734  0.21469568]\n"," [ 0.16011248 -0.48215601  0.32204353]]\n"]}]},{"cell_type":"markdown","source":["# 4.5 학습 알고리즘 구현하기\n","신경망 학습의 절차를 복습해보자.   \n","\n","__전제__   \n","신경망에 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과성을 학습이라고 한다.   \n","__1단계, 미니배치__   \n","훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라고 하며, 미니배치의 손실 함수 값을 줄이는 것이 목표이다.   \n","__2단계, 기울기 산출__   \n","미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.   \n","__3단계, 매개변수 갱신__   \n","가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.   \n","__4단계, 반복__   \n","1~3단계를 반복한다.\n","\n","경사 하강법으로 매개변수를 갱신하는 방법인데, 데이터를 미니배치로 무작위 선정이기 때문에 __확률적 경사 하강법(stochastic gradient descent, SGD)__ 이라고 한다.   \n"],"metadata":{"id":"hCiEr7QJ6THr"}},{"cell_type":"markdown","source":["### 4.5.1 2층 신경망 클래스 구현하기\n","MNIST 데이터셋과 2층 신경망(은닉층 1개)으로 손글씨 숫자를 학습하는 신경망을 구현해보자."],"metadata":{"id":"qc9iyD6E6V3_"}},{"cell_type":"code","source":["# import sys, os\n","# sys.path.append(os.pardir)\n","# from common.functions import *\n","# from common.gradient import numerical_gradient\n","import numpy as np\n","\n","# from common.functions import *\n","def identity_function(x):\n","    return x\n","\n","\n","def step_function(x):\n","    return np.array(x > 0, dtype=np.int)\n","\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))    \n","\n","\n","def sigmoid_grad(x):\n","    return (1.0 - sigmoid(x)) * sigmoid(x)\n","    \n","\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","\n","def relu_grad(x):\n","    grad = np.zeros_like(x)\n","    grad[x>=0] = 1\n","    return grad\n","    \n","\n","def softmax(x):\n","    x = x - np.max(x, axis=-1, keepdims=True)\n","    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n","\n","\n","def sum_squared_error(y, t):\n","    return 0.5 * np.sum((y-t)**2)\n","\n","\n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","        \n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","             \n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n","\n","\n","def softmax_loss(X, t):\n","    y = softmax(X)\n","    return cross_entropy_error(y, t)\n","    \n","# from common.gradient import numerical_gradient\n","def numerical_gradient(f, x):\n","    h = 1e-4 # 0.0001\n","    grad = np.zeros_like(x)\n","    \n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    while not it.finished:\n","        idx = it.multi_index\n","        tmp_val = x[idx]\n","        x[idx] = tmp_val + h\n","        fxh1 = f(x) # f(x+h)\n","        \n","        x[idx] = tmp_val - h \n","        fxh2 = f(x) # f(x-h)\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\n","        \n","        x[idx] = tmp_val\n","        it.iternext()   \n","        \n","    return grad"],"metadata":{"id":"S20MCvPQ8k8h","executionInfo":{"status":"ok","timestamp":1657161642634,"user_tz":-540,"elapsed":42,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["class TwoLayerNet:\n","    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n","        # weight init\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","        self.params['b2'] = np.zeros(output_size)\n","\n","    def predict(self, x):\n","        W1, W2 = self.params['W1'], self.params['W2']\n","        b1, b2 = self.params['b1'], self.params['b2']\n","\n","        a1 = np.dot(x, W1) + b1\n","        z1 = sigmoid(a1)\n","        a2 = np.dot(z1, W2) + b2\n","        y = softmax(a2)\n","\n","        return y\n","\n","    def loss(self, x, t):\n","        y = self.predict(x)\n","\n","        return cross_entropy_error(y, t)\n","    \n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        t = np.argmax(t, axis=1)\n","\n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","    \n","    def numerical_gradient(self, x, t):\n","        loss_W = lambda W: self.loss(x, t)\n","\n","        grads = {}\n","        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","\n","        return grads"],"metadata":{"id":"_2LkHcpe8-kS","executionInfo":{"status":"ok","timestamp":1657162882950,"user_tz":-540,"elapsed":526,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n","print(net.params['W1'].shape) # 784,100\n","print(net.params['b1'].shape) # 100,\n","print(net.params['W2'].shape) # 100,10\n","print(net.params['b2'].shape) # 10,"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"37lkeZchW2GH","executionInfo":{"status":"ok","timestamp":1657161642635,"user_tz":-540,"elapsed":42,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"0767c5b6-a5bb-4487-b715-c1baf459412e"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["(784, 100)\n","(100,)\n","(100, 10)\n","(10,)\n"]}]},{"cell_type":"code","source":["x = np.random.rand(100, 784) # 100장 28x28\n","y = net.predict(x)"],"metadata":{"id":"y1_NM3rIhuOj","executionInfo":{"status":"ok","timestamp":1657161642635,"user_tz":-540,"elapsed":37,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["params 변수는 신경망에 필요한 매개변수가 모두 저장된다.   \n","params 변수에 저장된 가중치 매개변수가 예측 처리(순방향 처리)에서 사용된다.(위 코드 참고)   \n","\n","grads 변수는 params 변수에 대응하는 각 매개변수의 기울기가 저장된다.(아래 코드 참고)   "],"metadata":{"id":"hYrT4aA4iFEo"}},{"cell_type":"code","source":["x = np.random.rand(100, 784)\n","t = np.random.rand(100, 10)\n","\n","grads = net.numerical_gradient(x, t) # 기울기 계산\n","\n","print(grads['W1'].shape) # 784,100\n","print(grads['b1'].shape) # 100,\n","print(grads['W2'].shape) # 100,10\n","print(grads['b2'].shape) # 10,"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rUbJS5qgjHtf","executionInfo":{"status":"ok","timestamp":1657161814292,"user_tz":-540,"elapsed":171694,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"d92302e6-60e6-4879-910a-8e8bde254432"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["(784, 100)\n","(100,)\n","(100, 10)\n","(10,)\n"]}]},{"cell_type":"markdown","source":["TwoLayerNet의 `__init__`은 가중치 매개변수도 초기화한다.   \n","가중치 매개변수의 초깃값을 무엇으로 설정하냐가 신경망 학습의 성공을 좌우하기도 한다.(이는 나중에 자세히 살펴보자.)   \n","지금은 정규분포를 따르는 난수(random number)로, 편향은 0으로 초기화했다.   \n","loss는 손실 함수의 값을 계산하는데, 교차 엔트로피 오차(CEE)로 구현했다.   \n","`numerical_gradient(self, x, t)` 메서드는 각 매개변수의 기울기를 계산한다.   "],"metadata":{"id":"DN9Uc_F-j8ho"}},{"cell_type":"markdown","source":["### 4.5.2 미니배치 학습 구현하기\n","MNIST 데이터셋과 TwoLayerNet 클래스로 학습을 수행해보자."],"metadata":{"id":"L7Yet7nR6Yr_"}},{"cell_type":"code","source":["# import numpy as np\n","# # from dataset.mnist import load_mnist\n","# # from two_layer_net import TwoLayerNet\n","\n","# (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","# train_loss_list = []\n","\n","# # hyperparameters\n","# iters_num = 10000\n","# train_size = x_train.shape[0]\n","# batch_size = 100\n","# learning_rate = 0.1\n","\n","# network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","# for i in range(iters_num):\n","#     batch_mask = np.random.choice(train_size, batch_size)\n","#     x_batch = x_train[batch_mask]\n","#     t_batch = t_train[batch_mask]\n","\n","#     grad = network.numerical_gradient(x_batch, t_batch)\n","    \n","#     for key in ('W1', 'b1', 'W2', 'b2'):\n","#         network.params[key] -= learning_rate * grad[key]\n","    \n","#     loss = network.loss(x_batch, t_batch)\n","#     train_loss_list.append(loss)"],"metadata":{"id":"smgwgRUWm0IJ","executionInfo":{"status":"ok","timestamp":1657161814293,"user_tz":-540,"elapsed":26,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["미니배치 크기를 100으로 했다.   \n","매번 6만개의 훈련 데이터에서 임의로 100개의 데이터를 추려낸다.   \n","100개의 미니배치를 대상으로 확률적 경사 하강법을 수행해 매개변수를 갱신한다.   \n","경사법에 의한 갱신 횟수를 1만번으로 설정하고, 갱신할 때마다 훈련 데이터에 대한 손실 함수를 계산하고, 그 값을 배열에 추가한다.   \n","이 손실 함수 값이 변화하는 추이를 그래프로 나타내면 다음과 같이 나온다.   \n","![](https://blog.kakaocdn.net/dn/1QBVl/btqIz4FyMJ3/dn3Vpqk51pgNyiIY8WOKOK/img.png)\n","\n","학습 횟수가 늘어가면서 손실 함수의 값이 줄어든다.   \n","이는 학습이 잘 되고 있다는 뜻으로, 신경망의 가중치 매개변수가 서서히 데이터에 적응하고 있음을 의미한다.   \n","__데이터를 반복해서 학습함으로써 최적 가중치 매개변수로 학습하고 있다.__"],"metadata":{"id":"0bysJthfqotw"}},{"cell_type":"markdown","source":["### 4.5.3 시험 데이터로 평가하기\n","위 그림의 loss는 훈련 과정에 있는 미니배치에 대한 손실 함수 값이다.   \n","신경망 학습에서는 훈련 데이터 외의 데이터를 올바르게 인식하는지 확인해야 한다.   \n","즉 오버피팅을 일으키지 않는지 확인해야 한다.   \n","\n","신경망 학습의 원래 목표는 범용적인 능력을 익히는 것이다.   \n","그래서 훈련 데이터에 포함되지 않은 데이터를 사용해 평가해야한다.\n","\n","평가가 제대로 이뤄질 수 있도록 위의 코드를 수정해보자."],"metadata":{"id":"Z-qPaQ2L6akw"}},{"cell_type":"code","source":["import numpy as np\n","# from dataset.mnist import load_mnist\n","# from two_layer_net import TwoLayerNet\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","# hyperparameters\n","iters_num = 10000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate = 0.1\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","\n","    grad = network.numerical_gradient(x_batch, t_batch)\n","    \n","    for key in ('W1', 'b1', 'W2', 'b2'):\n","        network.params[key] -= learning_rate * grad[key]\n","    \n","    loss = network.loss(x_batch, t_batch)\n","    train_loss_list.append(loss)\n","\n","    # 1 epoch마다 정확도 계산\n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(x_train, t_train)\n","        test_acc = network.accuracy(x_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","        print('train acc, test acc | ' + str(train_acc) + ', ' + str(test_acc))\n","\n","x = np.arange(len(train_acc_list))\n","plt.plot(x, train_acc_list, label='train acc')\n","plt.plot(x, test_acc_list, label='test acc', linestyple='--')\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.ylim(0, 1.0)\n","plt.legent(loc='lower right')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IHbOv_huuoXu","outputId":"ac16ffbc-3692-41da-d002-5552528e4eb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train acc, test acc | 0.09736666666666667, 0.0982\n"]}]},{"cell_type":"markdown","source":["1 epoch마다 모든 훈련 데이터와 시험 데이터에 대한 정확도를 계산하고, 그 결과를 기록했다.   \n","정확도를 1 epoch마다 계산하는 이유는 for문 안에서 매번 계산하기에 시간이 오래 걸리고, 또 그렇게까지 자주 기록할 필요도 없기 때문이다.\n","\n","그래프를 보면 학습이 진행될수록 훈련 데이터와 시험 데이터를 사용하고 평가한 정확도가 모두 좋아지고 있다.   \n","오버피팅도 일어나지 않았다."],"metadata":{"id":"K6trOvcs6alq"}},{"cell_type":"markdown","source":["# 4.6 정리\n","신경망 학습에 대해서 알아봤다.   \n","신경망이 학습할 수 있도록 손실 함수라는 지표를 도입하여, 이를 가장 작아지는 가중치 매개변수 값을 찾아내는 것이 신경망 학습의 목표이다.   \n","가능한 작은 손실 함수의 값을 찾는 수법으로 경사법을 알아봤다.   \n","경사법은 수치 미분을 통해 가중치 매개변수의 기울기를 구할 수 있다."],"metadata":{"id":"nGPswWM_-XBH"}}]}